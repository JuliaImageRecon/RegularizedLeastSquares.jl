var documenterSearchIndex = {"docs":
[{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"EditURL = \"../../literate/examples/computed_tomography.jl\"","category":"page"},{"location":"generated/examples/computed_tomography/#Computed-Tomography-Example","page":"Computed Tomography","title":"Computed Tomography Example","text":"","category":"section"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"In this example we will go through a simple example from the field of Computed Tomography. In addtion to RegularizedLeastSquares.jl, we will need the packages LinearOperatorCollection.jl, ImagePhantoms.jl, ImageGeoms.jl and RadonKA.jl, as well as CairoMakie.jl for visualization. We can install them the same way we did RegularizedLeastSquares.jl.","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"RadonKA is a package for the computation of the Radon transform and its adjoint. It is implemented with KernelAbstractions.jl and supports GPU acceleration. See the GPU acceleration how-to for more information.","category":"page"},{"location":"generated/examples/computed_tomography/#Preparing-the-Inverse-Problem","page":"Computed Tomography","title":"Preparing the Inverse Problem","text":"","category":"section"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"To get started, let us generate a simple phantom using the ImagePhantoms and ImageGeom packages:","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"using ImagePhantoms, ImageGeoms\nN = 256\nimage = shepp_logan(N, SheppLoganToft())\nsize(image)","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"This produces a 256x256 image of a Shepp-Logan phantom.","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"using RadonKA, LinearOperatorCollection\nangles = collect(range(0, π, 256))\nsinogram = Array(RadonKA.radon(image, angles))","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"Afterwards we build a Radon operator implementing both the forward and adjoint Radon transform","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"A = RadonOp(eltype(image); angles, shape = size(image));\nnothing #hide","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"To visualize our image we can use CairoMakie:","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"using CairoMakie\nfunction plot_image(figPos, img; title = \"\", width = 150, height = 150)\n  ax = CairoMakie.Axis(figPos[1, 1]; yreversed=true, title, width, height)\n  hidedecorations!(ax)\n  hm = heatmap!(ax, img)\n  Colorbar(figPos[2, 1], hm, vertical = false, flipaxis = false)\nend\nfig = Figure()\nplot_image(fig[1,1], image, title = \"Image\")\nplot_image(fig[1,2], sinogram, title = \"Sinogram\")\nplot_image(fig[1,3], backproject(sinogram, angles), title = \"Backprojection\")\nresize_to_layout!(fig)\nfig","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"In the figure we can see our original image, the sinogram, and the backprojection of the sinogram. The goal of the inverse problem is to recover the original image from the sinogram.","category":"page"},{"location":"generated/examples/computed_tomography/#Solving-the-Inverse-Problem","page":"Computed Tomography","title":"Solving the Inverse Problem","text":"","category":"section"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"To recover the image from the measurement vector, we solve the l^2_2-regularized least squares problem","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"beginequation\n  undersetmathbfxargmin frac12vertvert mathbfAmathbfx-mathbfb vertvert_2^2 + lambdavertvertmathbfxvertvert^2_2 \nendequation","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"For this purpose we build a l^2_2 with regularization parameter λ=0001","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"using RegularizedLeastSquares\nreg = L2Regularization(0.001);\nnothing #hide","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"To solve this inverse problem, the Conjugate Gradient Normal Residual (CGNR) algorithm can be used. This solver is based on the normal operator of the Radon operator and uses both the forward and adjoint Radon transform internally. We now build the corresponding solver","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"solver = createLinearSolver(CGNR, A; reg=reg, iterations=20);\nnothing #hide","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"and apply it to our measurement vector","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"img_approx = solve!(solver, vec(sinogram))","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"To visualize the reconstructed image, we need to reshape the result vector to the correct shape. Afterwards we can use CairoMakie again:","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"img_approx = reshape(img_approx,size(image));\nplot_image(fig[1,4], img_approx, title = \"Reconstructed Image\")\nresize_to_layout!(fig)\nfig","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"","category":"page"},{"location":"generated/examples/computed_tomography/","page":"Computed Tomography","title":"Computed Tomography","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"EditURL = \"../../literate/howto/efficient_kaczmarz.jl\"","category":"page"},{"location":"generated/howto/efficient_kaczmarz/#Efficient-Kaczmarz","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"","category":"section"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"Unlike many of the other solvers provided by RegularizedLeastSquares.jl, the Kaczmarz method does not utilize a matrix-vector product with the operator mathbfA nor the normal operator mathbfA*A. Instead, it uses the rows of mathbfA to update the solution iteratively. Efficient Kaczmarz implementation therefore require very efficient dot products with the rows of mathbfA. In RegularizedLeastSquares.jl, this is achieved with the dot_with_matrix_row function.","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"using RegularizedLeastSquares\nA = randn(256, 256)\nx = randn(256)\nb = A*x;\nnothing #hide","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"The dot_with_matrix_row function calculates the dot product between a row of A and the current approximate solution of x:","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"row = 1\nisapprox(RegularizedLeastSquares.dot_with_matrix_row(A, x, row), sum(A[row, :] .* x))","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"Since in Julia, dense arrays are stored in column-major order, such a row-based operation is quite inefficient. A workaround is to transpose the matrix then pass it to a Kaczmarz solver.","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"At = collect(transpose(A))\nA_eff = transpose(At)","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"Note that the transpose function can return a lazy transpose object, so we first collect the transpose into a dense matrix. Then we transpose it again to get the efficient representation of the matrix.","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"We can compare the performance using the BenchmarkTools.jl package. First for the original matrix:","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"using BenchmarkTools\nsolver = createLinearSolver(Kaczmarz, A; reg = L2Regularization(0.0001), iterations=100)\n@benchmark solve!(solver, b) samples = 100","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"And then for the efficient matrix:","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"solver_eff = createLinearSolver(Kaczmarz, A_eff; reg = L2Regularization(0.0001), iterations=100)\n@benchmark solve!(solver_eff, b) samples = 100","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"We can also combine the efficient matrix with a weighting matrix, as is shown in the Weighting example.","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"Custom operators need to implement the dot_with_matrix_row function to be used with the Kaczmarz solver. Ideally, such an implementation is allocation free.","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"","category":"page"},{"location":"generated/howto/efficient_kaczmarz/","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"EditURL = \"../../literate/howto/multi_threading.jl\"","category":"page"},{"location":"generated/howto/multi_threading/#Multi-Threading","page":"Multi-Threading","title":"Multi-Threading","text":"","category":"section"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"There are different ways multi-threading can be used with RegularizedLeastSquares.jl. To use multi-threading in Julia, one needs to start their session with multi-threads, see the Julia documentation for more information.","category":"page"},{"location":"generated/howto/multi_threading/#Solver-Based-Multi-Threading","page":"Multi-Threading","title":"Solver Based Multi-Threading","text":"","category":"section"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"This type of multi-threading is transparent to the solver and is applicable if the total solution is composed of individual solutions that can be solved in parallel. In particular, this approach also allows for using solvers with different parameters, such as their operator or regularization parameters.","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"using RegularizedLeastSquares\nAs = [rand(32, 16) for _ in 1:4]\nxs = [rand(16) for _ in 1:4]\nbs = [A*x for (A, x) in zip(As, xs)]\n\nxs_approx = similar(xs)\nThreads.@threads for i in 1:4\n  solver = createLinearSolver(CGNR, As[i]; iterations=32)\n  xs_approx[i] = solve!(solver, bs[i])\nend","category":"page"},{"location":"generated/howto/multi_threading/#Operator-Based-Multi-Threading","page":"Multi-Threading","title":"Operator Based Multi-Threading","text":"","category":"section"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"This type of multi-threading involves linear operators or proximal maps that can be implemnted in parallel. Examples of this include the proximal map of the TV regularization term, which is based on the multi-threaded GradientOp from LinearOperatorCollection. GPU acceleration also falls under this approach, see GPU Acceleration for more information.","category":"page"},{"location":"generated/howto/multi_threading/#Measurement-Based-Multi-Threading","page":"Multi-Threading","title":"Measurement Based Multi-Threading","text":"","category":"section"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"This level of multi-threading applies the same solver (and its parameters) to multiple measurement vectors or rather a measurement matrix B. This is useful in the case of multiple measurements that can be solved in parallel and can reuse the same solver. This approach is not applicable if the operator is stateful.","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"To use this approach we first build a measurement matrix B and a corresponding solver:","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"A = first(As)\nB = mapreduce(x -> A*x, hcat, xs)\nsolver = createLinearSolver(CGNR, A; iterations=32)","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"We can then simply pass the measurement matrix to the solver. The result will be the same as if we passed each colument of B seperately:","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"x_approx = solve!(solver, B)\nsize(x_approx)","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"The previous solve! call was still executed sequentially. To execute it in parallel, we have to specify a multi-threaded scheduler as a keyword-argument of the solve! call. RegularizedLeastSquares.jl provides a MultiThreadingState scheduler that can be used for this purpose. This scheduler is based on the Threads.@threads macro:","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"x_multi = solve!(solver, B; scheduler = MultiThreadingState)\nx_approx == x_multi","category":"page"},{"location":"generated/howto/multi_threading/#Custom-Scheduling","page":"Multi-Threading","title":"Custom Scheduling","text":"","category":"section"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"It is possible to implement custom scheduling. The following code shows how to implement this for the Threads.@spawn macro. Usually one this to implement multi-threading with a package such as FLoop.jl or ThreadPools.jl for thread pinning:","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"Since most solver have conv. criteria, they can finish at different iteration numbers, which we track this information with flags.","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":" mutable struct SpawnState{S, ST <: AbstractSolverState{S}} <: RegularizedLeastSquares.AbstractMatrixSolverState{S}\n   states::Vector{ST}\n   active::Vector{Bool}\n   SpawnState(states::Vector{ST}) where {S, ST <: AbstractSolverState{S}} = new{S, ST}(states, fill(true, length(states)))\n end","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"To hook into the existing init! code we only have to supply a method that gets a copyable \"vector\" state. This will invoke our SpawnState constructor with copies of the given state.","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":" prepareMultiStates(solver::AbstractLinearSolver, state::SpawnState, b::AbstractMatrix) = prepareMultiStates(solver, first(state.states), b);\nnothing #hide","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"We specialise the iterate function which is called with the idx of still active states","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"function Base.iterate(solver::AbstractLinearSolver, state::SpawnState, activeIdx)\n  @sync Threads.@spawn for i in activeIdx\n    res = iterate(solver, state.states[i])\n    if isnothing(res)\n      state.active[i] = false\n    end\n  end\n  return state.active, state\nend","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"Now we can simply use the SpawnState scheduler in the solve! call:","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"x_custom = solve!(solver, B; scheduler = SpawnState)\nx_approx == x_multi","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"","category":"page"},{"location":"generated/howto/multi_threading/","page":"Multi-Threading","title":"Multi-Threading","text":"This page was generated using Literate.jl.","category":"page"},{"location":"solvers/#Solvers","page":"Solvers","title":"Solvers","text":"","category":"section"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"RegularizedLeastSquares.jl provides a variety of solvers, which are used in fields such as MPI and MRI. The following is a non-exhaustive list of the implemented solvers:","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"Kaczmarz algorithm (Kaczmarz, also called Algebraic reconstruction technique)\nConjugate Gradients Normal Residual method (CGNR)\nFast Iterative Shrinkage Thresholding Algorithm (FISTA)\nAlternating Direction of Multipliers Method (ADMM)","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"The solvers are organized in a type-hierarchy and inherit from:","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"abstract type AbstractLinearSolver","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"The type hierarchy is further differentiated into solver categories such as AbstractRowAtionSolver, AbstractPrimalDualSolver or AbstractProximalGradientSolver. A list of all available solvers can be returned by the linearSolverList function.","category":"page"},{"location":"solvers/#Solver-Construction","page":"Solvers","title":"Solver Construction","text":"","category":"section"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"To create a solver, one can invoke the method createLinearSolver as in","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"solver = createLinearSolver(CGNR, A; reg=reg, kwargs...)","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"Here A denotes the operator and reg are the Regularization terms to be used by the solver. All further solver parameters can be passed as keyword arguments and are solver specific. To make things more compact, it can be usefull to collect all parameters in a Dict{Symbol,Any}. In this way, the code snippet above can be written as","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"params=Dict{Symbol,Any}()\nparams[:reg] = ...\n...\n\nsolver = createLinearSolver(CGNR, A; params...)","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"This notation can be convenient when a large number of parameters are set manually.","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"It is also possible to construct a solver directly with its specific keyword arguments:","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"solver = CGNR(A, reg = reg, ...)","category":"page"},{"location":"solvers/#Solver-Usage","page":"Solvers","title":"Solver Usage","text":"","category":"section"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"Once constructed, a solver can be used to approximate a solution to a given measurement vector:","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"x_approx = solve!(solver, b; kwargs...)","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"The keyword arguments can be used to supply an inital solution x0, one or more callbacks to interact and monitor the solvers state and more. See the How-To and the API for more information.","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"It is also possible to explicitly invoke the solvers iterations using Julias iterate interface:","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"init!(solver, b; kwargs...)\nfor (iteration, x_approx) in enumerate(solver)\n    println(\"Iteration $iteration\")\nend","category":"page"},{"location":"solvers/#Solver-Internals","page":"Solvers","title":"Solver Internals","text":"","category":"section"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"The fields of a solver can be divided into two groups. The first group are intended to be immutable fields that do not change during iterations, the second group are mutable fields that do change. Examples of the first group are the operator itself and examples of the second group are the current solution or the number of the current iteration.","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"The second group is usually encapsulated in its own state struct:","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"mutable struct Solver{matT, ...}\n  A::matT\n  # Other \"static\" fields\n  state::AbstractSolverState{<:Solver}\nend\n\nmutable struct SolverState{T, tempT} <: AbstractSolverState{Solver}\n  x::tempT\n  rho::T\n  # ...\n  iteration::Int64\nend","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"States are subtypes of the parametric AbstractSolverState{S} type. The state fields of solvers can be exchanged with different state belonging to the correct solver S. This means that the states can be used to realize custom variants of an existing solver:","category":"page"},{"location":"solvers/","page":"Solvers","title":"Solvers","text":"mutable struct VariantState{T, tempT} <: AbstractSolverState{Solver}\n  x::tempT\n  other::tempT\n  # ...\n  iteration::Int64\nend\n\nSolverVariant(A; kwargs...) = Solver(A, VariantState(kwargs...))\n\nfunction iterate(solver::Solver, state::VarianteState)\n  # Custom iteration\nend","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"EditURL = \"../../literate/howto/weighting.jl\"","category":"page"},{"location":"generated/howto/weighting/#Weighting","page":"Weighting","title":"Weighting","text":"","category":"section"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"Often time one wants to solve a weighted least squares problem of the form:","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"beginequation\n  undersetmathbfxargmin frac12vertvert mathbfAmathbfx-mathbfb vertvert^2_mathbfW + mathbfR(x) \nendequation","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"where mathbfW is a symmetric, positive weighting matrix and vertvertmathbfyvertvert^2_mathbfW denotes the weighted Euclidean norm. An example of such a weighting matrix is a noise whitening matrix. Another example could be a scaling of the matrix rows by the reciprocal of their row energy.","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"In the following, we will solve a weighted least squares problem of the form:","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"beginequation\n  undersetmathbfxargmin frac12vertvert mathbfAmathbfx-mathbfb vertvert_mathbfW^2 + lambdavertvertmathbfxvertvert^2_2 \nendequation","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"using RegularizedLeastSquares, LinearOperatorCollection, LinearAlgebra\nA = rand(32, 16)\nx = rand(16)\nb = A*x;\nnothing #hide","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"As a weighting matrix, we will use the reciprocal of the row energy of the matrix A.","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"weights = map(row -> 1/rownorm²(A, row), 1:size(A, 1));\nnothing #hide","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"First, let us solve the problem with matrices we manually weighted.","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"WA = diagm(weights) * A\nsolver = createLinearSolver(Kaczmarz, WA; reg = L2Regularization(0.0001), iterations=10)\nx_approx = solve!(solver, weights .* b);\nnothing #hide","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"The operator A is not always a dense matrix and the product between the operator and the weighting matrix is not always efficient or possible. The package LinearOperatorCollection.jl provides a matrix-free implementation of a diagonal weighting matrix, as well as a matrix free product between two matrices. This weighted operator has efficient implementations of the normal operator and also for the row-action operations of the Kaczmarz solver.","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"W = WeightingOp(weights)\nP = ProdOp(W, A)\nsolver = createLinearSolver(Kaczmarz, P; reg = L2Regularization(0.0001), iterations=10)\nx_approx2 = solve!(solver, W * b)\nisapprox(x_approx, x_approx2)","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"","category":"page"},{"location":"generated/howto/weighting/","page":"Weighting","title":"Weighting","text":"This page was generated using Literate.jl.","category":"page"},{"location":"API/regularization/#API-for-Regularizers","page":"Regularization Terms","title":"API for Regularizers","text":"","category":"section"},{"location":"API/regularization/","page":"Regularization Terms","title":"Regularization Terms","text":"This page contains documentation of the public API of the RegularizedLeastSquares. In the Julia REPL one can access this documentation by entering the help mode with ?","category":"page"},{"location":"API/regularization/","page":"Regularization Terms","title":"Regularization Terms","text":"RegularizedLeastSquares.L1Regularization\nRegularizedLeastSquares.L2Regularization\nRegularizedLeastSquares.L21Regularization\nRegularizedLeastSquares.LLRRegularization\nRegularizedLeastSquares.NuclearRegularization\nRegularizedLeastSquares.TVRegularization","category":"page"},{"location":"API/regularization/#RegularizedLeastSquares.L1Regularization","page":"Regularization Terms","title":"RegularizedLeastSquares.L1Regularization","text":"L1Regularization\n\nRegularization term implementing the proximal map for the Lasso problem.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.L2Regularization","page":"Regularization Terms","title":"RegularizedLeastSquares.L2Regularization","text":"L2Regularization\n\nRegularization term implementing the proximal map for Tikhonov regularization.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.L21Regularization","page":"Regularization Terms","title":"RegularizedLeastSquares.L21Regularization","text":"L21Regularization\n\nRegularization term implementing the proximal map for group-soft-thresholding.\n\nArguments\n\nλ                  - regularization paramter\n\nKeywords\n\nslices=1           - number of elements per group\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.LLRRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.LLRRegularization","text":"LLRRegularization\n\nRegularization term implementing the proximal map for locally low rank (LLR) regularization using singular-value-thresholding.\n\nArguments\n\nλ                  - regularization paramter\n\nKeywords\n\nshape::Tuple{Int}            - dimensions of the image\nblockSize::Tuple{Int}=(2,2)  - size of patches to perform singular value thresholding on\nrandshift::Bool=true         - randomly shifts the patches to ensure translation invariance\nfullyOverlapping::Bool=false - choose between fully overlapping block or non-overlapping blocks\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.NuclearRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.NuclearRegularization","text":"NuclearRegularization\n\nRegularization term implementing the proximal map for singular value soft-thresholding.\n\nArguments:\n\nλ           - regularization paramter\n\nKeywords\n\nsvtShape::NTuple  - size of the underlying matrix\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.TVRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.TVRegularization","text":"TVRegularization\n\nRegularization term implementing the proximal map for TV regularization. Calculated with the Condat algorithm if the TV is calculated only along one real-valued dimension and with the Fast Gradient Projection algorithm otherwise.\n\nReference for the Condat algorithm: https://lcondat.github.io/publis/Condat-fast_TV-SPL-2013.pdf\n\nReference for the FGP algorithm: A. Beck and T. Teboulle, \"Fast Gradient-Based Algorithms for Constrained Total Variation Image Denoising and Deblurring Problems\", IEEE Trans. Image Process. 18(11), 2009\n\nArguments\n\nλ::T                    - regularization parameter\n\nKeywords\n\nshape::NTuple           - size of the underlying image\ndims                    - Dimension to perform the TV along. If Integer, the Condat algorithm is called, and the FDG algorithm otherwise.\niterationsTV=20         - number of FGP iterations\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#Projection-Regularization","page":"Regularization Terms","title":"Projection Regularization","text":"","category":"section"},{"location":"API/regularization/","page":"Regularization Terms","title":"Regularization Terms","text":"RegularizedLeastSquares.PositiveRegularization\nRegularizedLeastSquares.RealRegularization","category":"page"},{"location":"API/regularization/#RegularizedLeastSquares.PositiveRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.PositiveRegularization","text":"PositiveRegularization\n\nRegularization term implementing a projection onto positive and real numbers.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.RealRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.RealRegularization","text":"RealRegularization\n\nRegularization term implementing a projection onto real numbers.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#Nested-Regularization","page":"Regularization Terms","title":"Nested Regularization","text":"","category":"section"},{"location":"API/regularization/","page":"Regularization Terms","title":"Regularization Terms","text":"RegularizedLeastSquares.innerreg(::AbstractNestedRegularization)\nRegularizedLeastSquares.sink(::AbstractNestedRegularization)\nRegularizedLeastSquares.sinktype(::AbstractNestedRegularization)","category":"page"},{"location":"API/regularization/#RegularizedLeastSquares.innerreg-Tuple{AbstractNestedRegularization}","page":"Regularization Terms","title":"RegularizedLeastSquares.innerreg","text":"innerreg(reg::AbstractNestedRegularization)\n\nreturn the inner regularization term of reg. Nested regularization terms also implement the iteration interface.\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#RegularizedLeastSquares.sink-Tuple{AbstractNestedRegularization}","page":"Regularization Terms","title":"RegularizedLeastSquares.sink","text":"sink(reg::AbstractNestedRegularization)\n\nreturn the innermost regularization term.\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#RegularizedLeastSquares.sinktype-Tuple{AbstractNestedRegularization}","page":"Regularization Terms","title":"RegularizedLeastSquares.sinktype","text":"sinktype(reg::AbstractNestedRegularization)\n\nreturn the type of the innermost regularization term.\n\nSee also sink.\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#Scaled-Regularization","page":"Regularization Terms","title":"Scaled Regularization","text":"","category":"section"},{"location":"API/regularization/","page":"Regularization Terms","title":"Regularization Terms","text":"RegularizedLeastSquares.AbstractScaledRegularization\nRegularizedLeastSquares.scalefactor\nRegularizedLeastSquares.NormalizedRegularization\nRegularizedLeastSquares.NoNormalization\nRegularizedLeastSquares.MeasurementBasedNormalization\nRegularizedLeastSquares.SystemMatrixBasedNormalization\nRegularizedLeastSquares.FixedParameterRegularization","category":"page"},{"location":"API/regularization/#RegularizedLeastSquares.AbstractScaledRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.AbstractScaledRegularization","text":"AbstractScaledRegularization\n\nNested regularization term that applies a scalefactor to the regularization parameter λ of its inner term.\n\nSee also scalefactor, λ, innerreg.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.scalefactor","page":"Regularization Terms","title":"RegularizedLeastSquares.scalefactor","text":"scalescalefactor(reg::AbstractScaledRegularization)\n\nreturn the scaling scalefactor for λ\n\n\n\n\n\n","category":"function"},{"location":"API/regularization/#RegularizedLeastSquares.NormalizedRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.NormalizedRegularization","text":"NormalizedRegularization\n\nNested regularization term that scales λ according to normalization scheme. This term is commonly applied by a solver based on a given normalization keyword\n\n#See also NoNormalization, MeasurementBasedNormalization, SystemMatrixBasedNormalization.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.NoNormalization","page":"Regularization Terms","title":"RegularizedLeastSquares.NoNormalization","text":"NoNormalization\n\nNo normalization to λ is applied.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.MeasurementBasedNormalization","page":"Regularization Terms","title":"RegularizedLeastSquares.MeasurementBasedNormalization","text":"MeasurementBasedNormalization\n\nλ is normalized by the 1-norm of b divided by its length.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.SystemMatrixBasedNormalization","page":"Regularization Terms","title":"RegularizedLeastSquares.SystemMatrixBasedNormalization","text":"SystemMatrixBasedNormalization\n\nλ is normalized by the energy of the system matrix rows.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.FixedParameterRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.FixedParameterRegularization","text":"FixedParameterRegularization\n\nNested regularization term that discards any λ passed to it and instead uses λ from its inner regularization term. This can be used to selectively disallow normalization.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#Misc.-Nested-Regularization","page":"Regularization Terms","title":"Misc. Nested Regularization","text":"","category":"section"},{"location":"API/regularization/","page":"Regularization Terms","title":"Regularization Terms","text":"RegularizedLeastSquares.MaskedRegularization\nRegularizedLeastSquares.TransformedRegularization\nRegularizedLeastSquares.PlugAndPlayRegularization","category":"page"},{"location":"API/regularization/#RegularizedLeastSquares.MaskedRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.MaskedRegularization","text":"MaskedRegularization\n\nNested regularization term that only applies prox! and norm to elements of x for which the mask is true.\n\nExamples\n\njulia> positive = PositiveRegularization();\n\njulia> masked = MaskedRegularization(reg, [true, false, true, false]);\n\njulia> prox!(masked, fill(-1, 4))\n4-element Vector{Float64}:\n  0.0\n -1.0\n  0.0\n -1.0\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.TransformedRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.TransformedRegularization","text":"TransformedRegularization(reg, trafo)\n\nNested regularization term that applies prox! or norm on z = trafo * x and returns (inplace) x = adjoint(trafo) * z.\n\nExample\n\njulia> core = L1Regularization(0.8)\nL1Regularization{Float64}(0.8)\n\njulia> wop = WaveletOp(Float32, shape = (32,32));\n\njulia> reg = TransformedRegularization(core, wop);\n\njulia> prox!(reg, randn(32*32)); # Apply soft-thresholding in Wavelet domain\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.PlugAndPlayRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.PlugAndPlayRegularization","text":"    PlugAndPlayRegularization\n\nRegularization term implementing a given plug-and-play proximal mapping. The actual regularization term is indirectly defined by the learned proximal mapping and as such there is no norm implemented.\n\nArguments\n\nλ                  - regularization paramter\n\nKeywords\n\nmodel       - model applied to the image\nshape       - dimensions of the image\ninput_transform - transform of image before model\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#Miscellaneous-Functions","page":"Regularization Terms","title":"Miscellaneous Functions","text":"","category":"section"},{"location":"API/regularization/","page":"Regularization Terms","title":"Regularization Terms","text":"RegularizedLeastSquares.prox!(::AbstractParameterizedRegularization, ::AbstractArray)\nRegularizedLeastSquares.prox!(::Type{<:AbstractParameterizedRegularization}, ::Any, ::Any)\nRegularizedLeastSquares.norm(::AbstractParameterizedRegularization, ::AbstractArray)\nRegularizedLeastSquares.λ(::AbstractParameterizedRegularization)\nRegularizedLeastSquares.norm(::Type{<:AbstractParameterizedRegularization}, ::Any, ::Any)","category":"page"},{"location":"API/regularization/#RegularizedLeastSquares.prox!-Tuple{AbstractParameterizedRegularization, AbstractArray}","page":"Regularization Terms","title":"RegularizedLeastSquares.prox!","text":"prox!(reg::AbstractParameterizedRegularization, x)\n\nperform the proximal mapping defined by reg on x. Uses the regularization parameter defined for reg.\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#RegularizedLeastSquares.prox!-Tuple{Type{<:AbstractParameterizedRegularization}, Any, Any}","page":"Regularization Terms","title":"RegularizedLeastSquares.prox!","text":"prox!(regType::Type{<:AbstractParameterizedRegularization}, x, λ; kwargs...)\n\nconstruct a regularization term of type regType with given λ and kwargs and apply its prox! on x\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#LinearAlgebra.norm-Tuple{AbstractParameterizedRegularization, AbstractArray}","page":"Regularization Terms","title":"LinearAlgebra.norm","text":"norm(reg::AbstractParameterizedRegularization, x)\n\nreturns the value of the reg regularization term on x. Uses the regularization parameter defined for reg.\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#RegularizedLeastSquares.λ-Tuple{AbstractParameterizedRegularization}","page":"Regularization Terms","title":"RegularizedLeastSquares.λ","text":"λ(reg::AbstractParameterizedRegularization)\n\nreturn the regularization parameter λ of reg\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#LinearAlgebra.norm-Tuple{Type{<:AbstractParameterizedRegularization}, Any, Any}","page":"Regularization Terms","title":"LinearAlgebra.norm","text":"norm(regType::Type{<:AbstractParameterizedRegularization}, x, λ; kwargs...)\n\nconstruct a regularization term of type regType with given λ and kwargs and apply its norm on x\n\n\n\n\n\n","category":"method"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"EditURL = \"../../literate/howto/callbacks.jl\"","category":"page"},{"location":"generated/howto/callbacks/#Callbacks","page":"Callbacks","title":"Callbacks","text":"","category":"section"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"For certain optimization it is important to monitor the internal state of the solver. RegularizedLeastSquares.jl provides a callback mechanism to allow developres to access this state after each iteration. The package provides a variety of default callbacks, which for example store the solution after each iteration. More information can be found in the API reference for the solve! function.","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"In this example we will revist the compressed sensing compressed sensing example and implement a custom callback using the do-syntax of the solve! function. This allows us to implement our callback inline and access the solver state after each iteration.","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"We first recreate the operator A and the measurement vector b:","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"using ImagePhantoms, ImageGeoms\nN = 256\nimage = shepp_logan(N, SheppLoganToft())\nsize(image)\nusing Random, LinearOperatorCollection\nrandomIndices = shuffle(eachindex(image))\nsampledIndices = sort(randomIndices[1:div(end, 3)])\nA = SamplingOp(eltype(image), pattern = sampledIndices , shape = size(image));\nb = A*vec(image);\nnothing #hide","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"Next we prepare our visualization helper function:","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"using CairoMakie\nfunction plot_image(figPos, img; title = \"\", width = 150, height = 150, clim = extrema(img))\n  ax = CairoMakie.Axis(figPos; yreversed=true, title, width, height)\n  hidedecorations!(ax)\n  heatmap!(ax, img, colorrange = clim)\nend","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"Now we construct the solver with the TV regularization term:","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"using RegularizedLeastSquares\nreg = TVRegularization(0.01; shape=size(image));\nsolver = createLinearSolver(FISTA, A; reg=reg, iterations=20);\nnothing #hide","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"We will now implement a callback that plots the solution every four iteration:","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"fig = Figure()\nidx = 1\nsolve!(solver, b) do solver, iteration\n  if iteration % 4 == 0\n    img_approx = copy(solversolution(solver))\n    img_approx = reshape(img_approx, size(image))\n    plot_image(fig[div(idx -1, 3) + 1, mod1(idx, 3)], img_approx, clim = extrema(image), title = \"$iteration\")\n    global idx += 1\n  end\nend","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"In the callback we have to copy the solution, as the solver will update it in place. As is explained in the solver section, each features fields that are intended to be immutable during a solve! call and a state that is modified in each iteration. Depending on the solvers parameters and the measurement input, the state can differ in its fields and their type. Ideally, one tries to avoid accessing the state directly and uses the provided functions to access the state.","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"The resulting figure shows the reconstructed image after 0, 4, 8, 12, 16 and 20 iterations:","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"resize_to_layout!(fig)\nfig","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"","category":"page"},{"location":"generated/howto/callbacks/","page":"Callbacks","title":"Callbacks","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"EditURL = \"../../literate/howto/normal_operator.jl\"","category":"page"},{"location":"generated/howto/normal_operator/#Normal-Operator","page":"Normal Operator","title":"Normal Operator","text":"","category":"section"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"Many solvers in RegularizedLeastSquares.jl are based on the normal operator mathbfA^*mathbfA of the linear operator mathbfA.","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"Solvers such as ADMM, FISTA and POGM generally solve optimization problems of the form:","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"beginequation\n  undersetmathbfxargmin mathbff(x)+ mathbfR(x)\nendequation","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"and require the gradient of the function mathbff(x). In this package we specialise the function mathbff(x) to the least squares norm:","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"beginequation\n  mathbff(x) = frac12vertvert mathbfAmathbfx-mathbfb vertvert^2_2\nendequation","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"The gradient of this function is:","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"beginequation\n  nabla mathbff(x) = mathbfA^*(mathbfAmathbfx-mathbfb) = mathbfA^*mathbfAx - mathbfA^*mathbfb\nendequation","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"Similarily, the conjugate gradient normal residual (CGNR) algorithm applies conjugate gradient algorithm to:","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"beginequation\n  mathbfA^*mathbfAmathbfx = mathbfA^*mathbfb\nendequation","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"The normal operator can be passed directly to these solvers, otherwise it is computed internally.","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"using RegularizedLeastSquares\nA = randn(32, 16)\nx = randn(16)\nb = A*x\n\nsolver = createLinearSolver(CGNR, A; AHA = adjoint(A) * A, reg = L2Regularization(0.0001), iterations=32);\nx_approx = solve!(solver, b)","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"The normal operator can also be computed using the normalOperator function from LinearOperatorCollection.jl. This is useful if the normal operator is not directly available or shouldn't be stored in memory. This function is opinionated and attempts to optimize the resulting operator for iterative applications. Specifying a custom method for a custom operator allows one to control this optimization.","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"An example of such an optimization is a matrix-free weighting of mathbfA as shown in the Weighting example:","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"using LinearOperatorCollection\nweights = rand(32)\nWA = ProdOp(WeightingOp(weights), A)\nAHA = LinearOperatorCollection.normalOperator(WA)","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"Without an optimization a matrix-free product would apply the following operator each iteration:","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"beginequation\n  (mathbfWA)^*mathbfWA = mathbfA^*mathbfW^*mathbfWmathbfA\nendequation","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"This is not efficient and instead the normal operator can be optimized by initially computing the weights:","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"beginequation\n  tildemathbfW = mathbfW^*mathbfW\nendequation","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"and then applying the following each iteration:","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"beginequation\n  mathbfA^*tildemathbfWmathbfA\nendequation","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"The optimized normal operator can then be passed to the solver:","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"solver = createLinearSolver(CGNR, WA; AHA = AHA, reg = L2Regularization(0.0001), iterations=32);\nx_approx2 = solve!(solver, weights .* b)","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"Of course it is also possible to optimize a normal operator with other means and pass it to the solver via the AHA keyword argument.","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"It is also possible to only supply the normal operator to these solvers, however on then needs to supply mathbfA^*b intead of mathbfb.","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"","category":"page"},{"location":"generated/howto/normal_operator/","page":"Normal Operator","title":"Normal Operator","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"EditURL = \"../../literate/howto/plug-and-play.jl\"","category":"page"},{"location":"generated/howto/plug-and-play/#Plug-and-Play-Regularization","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"","category":"section"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"A group of regularization terms that can not be directly written down as function are learned plug-and-play (PnP) priors. These are terms based on deep neural networks, which are trainted to implement the proximal map corresponding to the regularization term. Such a PnP prior can be used in the same way as any other regularization term.","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"The following example shows how to use a PnP prior in the context of the Kaczmarz solver.","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"using RegularizedLeastSquares\nA = randn(32, 16)\nx = randn(16)\nb = A*x;\nnothing #hide","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"For the documentation we will just use the identity function as a placeholder for the PnP prior.","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"model = identity","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"In practice, you would replace this with a neural network:","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"using Flux\nmodel = Flux.loadmodel!(model, ...)","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"The model can then be used together with the PnPRegularization term:","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"reg = PnPRegularization(1.0; model = model, shape = [16]);\nnothing #hide","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"Since models often expect a specific input range, we can use the MinMaxTransform to normalize the input:","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"reg = PnPRegularization(1.0; model = model, shape = [16], input_transform = RegularizedLeastSquares.MinMaxTransform);\nnothing #hide","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"Custom input transforms can be implemented by passing something callable as the input_transform keyword argument. For more details see the PnPRegularization documentation.","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"The regularization term can then be used in the solver:","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"solver = createLinearSolver(Kaczmarz, A; reg = reg, iterations = 32)\nx_approx = solve!(solver, b)","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"","category":"page"},{"location":"generated/howto/plug-and-play/","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"This page was generated using Literate.jl.","category":"page"},{"location":"API/solvers/#API-for-Solvers","page":"Solvers","title":"API for Solvers","text":"","category":"section"},{"location":"API/solvers/","page":"Solvers","title":"Solvers","text":"This page contains documentation of the public API of the RegularizedLeastSquares. In the Julia REPL one can access this documentation by entering the help mode with ?","category":"page"},{"location":"API/solvers/#solve!","page":"Solvers","title":"solve!","text":"","category":"section"},{"location":"API/solvers/","page":"Solvers","title":"Solvers","text":"RegularizedLeastSquares.solve!(::AbstractLinearSolver, ::Any)\nRegularizedLeastSquares.init!(::AbstractLinearSolver, ::Any)\nRegularizedLeastSquares.init!(::AbstractLinearSolver, ::AbstractSolverState, ::AbstractMatrix)\n","category":"page"},{"location":"API/solvers/#RegularizedLeastSquares.solve!-Tuple{AbstractLinearSolver, Any}","page":"Solvers","title":"RegularizedLeastSquares.solve!","text":"solve!(solver::AbstractLinearSolver, b; x0 = 0, callbacks = (_, _) -> nothing)\n\nSolves an inverse problem for the data vector b using solver.\n\nRequired Arguments\n\nsolver::AbstractLinearSolver    - linear solver (e.g., ADMM or FISTA), containing forward/normal operator and regularizer\nb::AbstractVector               - data vector if A was supplied to the solver, back-projection of the data otherwise\n\nOptional Keyword Arguments\n\nx0::AbstractVector              - initial guess for the solution; default is zero\ncallbacks              - (optionally a vector of) function or callable struct that takes the two arguments callback(solver, iteration) and, e.g., stores, prints, or plots the intermediate solutions or convergence parameters. Be sure not to modify solver or iteration in the callback function as this would japaridze convergence. The default does nothing.\n\nExamples\n\nThe optimization problem\n\n\targmin_x Ax - b_2^2 + λ x_1\n\ncan be solved with the following lines of code:\n\njulia> using RegularizedLeastSquares\n\njulia> A = [0.831658  0.96717\n            0.383056  0.39043\n            0.820692  0.08118];\n\njulia> x = [0.5932234523399985; 0.2697534345340015];\n\njulia> b = A * x;\n\njulia> S = ADMM(A);\n\njulia> x_approx = solve!(S, b)\n2-element Vector{Float64}:\n 0.5932234523399984\n 0.26975343453400163\n\nHere, we use L1Regularization, which is default for ADMM. All regularization options can be found in API for Regularizers.\n\nThe following example solves the same problem, but stores the solution x of each interation in tr:\n\njulia> tr = Dict[]\nDict[]\n\njulia> store_trace!(tr, solver, iteration) = push!(tr, Dict(\"iteration\" => iteration, \"x\" => solver.x, \"beta\" => solver.β))\nstore_trace! (generic function with 1 method)\n\njulia> x_approx = solve!(S, b; callbacks=(solver, iteration) -> store_trace!(tr, solver, iteration))\n2-element Vector{Float64}:\n 0.5932234523399984\n 0.26975343453400163\n\njulia> tr[3]\nDict{String, Any} with 3 entries:\n  \"iteration\" => 2\n  \"x\"         => [0.593223, 0.269753]\n  \"beta\"      => [1.23152, 0.927611]\n\nThe last example show demonstrates how to plot the solution at every 10th iteration and store the solvers convergence metrics:\n\njulia> using Plots\n\njulia> conv = StoreConvergenceCallback()\n\njulia> function plot_trace(solver, iteration)\n         if iteration % 10 == 0\n           display(scatter(solver.x))\n         end\n       end\nplot_trace (generic function with 1 method)\n\njulia> x_approx = solve!(S, b; callbacks = [conv, plot_trace]);\n\nThe keyword callbacks allows you to pass a (vector of) callable objects that takes the arguments solver and iteration and prints, stores, or plots intermediate result.\n\nSee also StoreSolutionCallback, StoreConvergenceCallback, CompareSolutionCallback for a number of provided callback options.\n\n\n\n\n\n","category":"method"},{"location":"API/solvers/#RegularizedLeastSquares.init!-Tuple{AbstractLinearSolver, Any}","page":"Solvers","title":"RegularizedLeastSquares.init!","text":"init!(solver::AbstractLinearSolver, b; kwargs...)\n\nPrepare the solver for iteration based on the given data vector b and kwargs.\n\n\n\n\n\n","category":"method"},{"location":"API/solvers/#RegularizedLeastSquares.init!-Tuple{AbstractLinearSolver, AbstractSolverState, AbstractMatrix}","page":"Solvers","title":"RegularizedLeastSquares.init!","text":"init!(solver::AbstractLinearSolver, state::AbstractSolverState, b::AbstractMatrix; scheduler = SequentialState, kwargs...)\n\nInitialize the solver with each column of b and pass the corresponding states to the scheduler.\n\n\n\n\n\n","category":"method"},{"location":"API/solvers/#ADMM","page":"Solvers","title":"ADMM","text":"","category":"section"},{"location":"API/solvers/","page":"Solvers","title":"Solvers","text":"RegularizedLeastSquares.ADMM","category":"page"},{"location":"API/solvers/#RegularizedLeastSquares.ADMM","page":"Solvers","title":"RegularizedLeastSquares.ADMM","text":"ADMM(A; AHA = A'*A, precon = Identity(), reg = L1Regularization(zero(real(eltype(AHA)))), regTrafo = opEye(eltype(AHA), size(AHA,1)), normalizeReg = NoNormalization(), rho = 1e-1, vary_rho = :none, iterations = 10, iterationsCG = 10, absTol = eps(real(eltype(AHA))), relTol = eps(real(eltype(AHA))), tolInner = 1e-5, verbose = false)\nADMM( ; AHA = ,     precon = Identity(), reg = L1Regularization(zero(real(eltype(AHA)))), regTrafo = opEye(eltype(AHA), size(AHA,1)), normalizeReg = NoNormalization(), rho = 1e-1, vary_rho = :none, iterations = 10, iterationsCG = 10, absTol = eps(real(eltype(AHA))), relTol = eps(real(eltype(AHA))), tolInner = 1e-5, verbose = false)\n\nCreates an ADMM object for the forward operator A or normal operator AHA.\n\nRequired Arguments\n\nA                                                 - forward operator\n\nOR\n\nAHA                                               - normal operator (as a keyword argument)\n\nOptional Keyword Arguments\n\nAHA                                               - normal operator is optional if A is supplied\nprecon                                            - preconditionner for the internal CG algorithm\nreg::AbstractParameterizedRegularization          - regularization term; can also be a vector of regularization terms\nregTrafo                                          - transformation to a space in which reg is applied; if reg is a vector, regTrafo has to be a vector of the same length. Use opEye(eltype(AHA), size(AHA,1)) if no transformation is desired.\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\nrho::Real                                         - penalty of the augmented Lagrangian\nvary_rho::Symbol                                  - vary rho to balance primal and dual feasibility; options :none, :balance, :PnP\niterations::Int                                   - maximum number of (outer) ADMM iterations\niterationsCG::Int                                 - maximum number of (inner) CG iterations\nabsTol::Real                                      - absolute tolerance for stopping criterion\nrelTol::Real                                      - relative tolerance for stopping criterion\ntolInner::Real                                    - relative tolerance for CG stopping criterion\nverbose::Bool                                     - print residual in each iteration\n\nADMM differs from ISTA-type algorithms in the sense that the proximal operation is applied separately from the transformation to the space in which the penalty is applied. This is reflected by the interface which has reg and regTrafo as separate arguments. E.g., for a TV penalty, you should NOT set reg=TVRegularization, but instead use reg=L1Regularization(λ), regTrafo=RegularizedLeastSquares.GradientOp(Float64; shape=(Nx,Ny,Nz)).\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#CGNR","page":"Solvers","title":"CGNR","text":"","category":"section"},{"location":"API/solvers/","page":"Solvers","title":"Solvers","text":"RegularizedLeastSquares.CGNR","category":"page"},{"location":"API/solvers/#RegularizedLeastSquares.CGNR","page":"Solvers","title":"RegularizedLeastSquares.CGNR","text":"CGNR(A; AHA = A' * A, reg = L2Regularization(zero(real(eltype(AHA)))), normalizeReg = NoNormalization(), iterations = 10, relTol = eps(real(eltype(AHA))))\nCGNR( ; AHA = ,       reg = L2Regularization(zero(real(eltype(AHA)))), normalizeReg = NoNormalization(), iterations = 10, relTol = eps(real(eltype(AHA))))\n\ncreates an CGNR object for the forward operator A or normal operator AHA.\n\nRequired Arguments\n\nA                                                 - forward operator\n\nOR\n\nAHA                                               - normal operator (as a keyword argument)\n\nOptional Keyword Arguments\n\nAHA                                               - normal operator is optional if A is supplied\nreg::AbstractParameterizedRegularization          - regularization term; can also be a vector of regularization terms\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\niterations::Int                                   - maximum number of iterations\nrelTol::Real                                      - tolerance for stopping criterion\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#Kaczmarz","page":"Solvers","title":"Kaczmarz","text":"","category":"section"},{"location":"API/solvers/","page":"Solvers","title":"Solvers","text":"RegularizedLeastSquares.Kaczmarz","category":"page"},{"location":"API/solvers/#RegularizedLeastSquares.Kaczmarz","page":"Solvers","title":"RegularizedLeastSquares.Kaczmarz","text":"Kaczmarz(A; reg = L2Regularization(0), normalizeReg = NoNormalization(), randomized=false, subMatrixFraction=0.15, shuffleRows=false, seed=1234, iterations=10)\n\nCreates a Kaczmarz object for the forward operator A.\n\nRequired Arguments\n\nA                                                 - forward operator\n\nOptional Keyword Arguments\n\nreg::AbstractParameterizedRegularization          - regularization term\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\nrandomized::Bool                                    - randomize Kacmarz algorithm\nsubMatrixFraction::Real                             - fraction of rows used in randomized Kaczmarz algorithm\nshuffleRows::Bool                                   - randomize Kacmarz algorithm\nseed::Int                                           - seed for randomized algorithm\niterations::Int                                     - number of iterations\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#FISTA","page":"Solvers","title":"FISTA","text":"","category":"section"},{"location":"API/solvers/","page":"Solvers","title":"Solvers","text":"RegularizedLeastSquares.FISTA","category":"page"},{"location":"API/solvers/#RegularizedLeastSquares.FISTA","page":"Solvers","title":"RegularizedLeastSquares.FISTA","text":"FISTA(A; AHA=A'*A, reg=L1Regularization(zero(real(eltype(AHA)))), normalizeReg=NoNormalization(), iterations=50, verbose = false, rho = 0.95 / power_iterations(AHA), theta=1, relTol=eps(real(eltype(AHA))), restart = :none)\nFISTA( ; AHA=,     reg=L1Regularization(zero(real(eltype(AHA)))), normalizeReg=NoNormalization(), iterations=50, verbose = false, rho = 0.95 / power_iterations(AHA), theta=1, relTol=eps(real(eltype(AHA))), restart = :none)\n\ncreates a FISTA object for the forward operator A or normal operator AHA.\n\nRequired Arguments\n\nA                                                 - forward operator\n\nOR\n\nAHA                                               - normal operator (as a keyword argument)\n\nOptional Keyword Arguments\n\nAHA                                               - normal operator is optional if A is supplied\nprecon                                            - preconditionner for the internal CG algorithm\nreg::AbstractParameterizedRegularization          - regularization term; can also be a vector of regularization terms\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\nrho::Real                                         - step size for gradient step; the default is 0.95 / max_eigenvalue as determined with power iterations.\ntheta::Real                                       - parameter for predictor-corrector step\nrelTol::Real                                      - tolerance for stopping criterion\niterations::Int                                   - maximum number of iterations\nrestart::Symbol                                   - :none, :gradient options for restarting\nverbose::Bool                                     - print residual in each iteration\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#OptISTA","page":"Solvers","title":"OptISTA","text":"","category":"section"},{"location":"API/solvers/","page":"Solvers","title":"Solvers","text":"RegularizedLeastSquares.OptISTA","category":"page"},{"location":"API/solvers/#RegularizedLeastSquares.OptISTA","page":"Solvers","title":"RegularizedLeastSquares.OptISTA","text":"OptISTA(A; AHA=A'*A, reg=L1Regularization(zero(real(eltype(AHA)))), normalizeReg=NoNormalization(), iterations=50, verbose = false, rho=0.95 / power_iterations(AHA), theta=1, relTol=eps(real(eltype(AHA))))\nOptISTA( ; AHA=,     reg=L1Regularization(zero(real(eltype(AHA)))), normalizeReg=NoNormalization(), iterations=50, verbose = false, rho=0.95 / power_iterations(AHA), theta=1, relTol=eps(real(eltype(AHA))))\n\ncreates a OptISTA object for the forward operator A or normal operator AHA. OptISTA has a 2x better worst-case bound than FISTA, but actual performance varies by application. It stores 2 extra intermediate variables the size of the image compared to FISTA.\n\nReference:\n\nUijeong Jang, Shuvomoy Das Gupta, Ernest K. Ryu, \"Computer-Assisted Design of Accelerated Composite Optimization Methods: OptISTA,\" arXiv:2305.15704, 2023, [https://arxiv.org/abs/2305.15704]\n\nRequired Arguments\n\nA                                                 - forward operator\n\nOR\n\nAHA                                               - normal operator (as a keyword argument)\n\nOptional Keyword Arguments\n\nAHA                                               - normal operator is optional if A is supplied\nreg::AbstractParameterizedRegularization          - regularization term\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\nrho::Real                                         - step size for gradient step; the default is 0.95 / max_eigenvalue as determined with power iterations.\ntheta::Real                                       - parameter for predictor-corrector step\nrelTol::Real                                      - tolerance for stopping criterion\niterations::Int                                   - maximum number of iterations\nverbose::Bool                                     - print residual in each iteration\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#POGM","page":"Solvers","title":"POGM","text":"","category":"section"},{"location":"API/solvers/","page":"Solvers","title":"Solvers","text":"RegularizedLeastSquares.POGM","category":"page"},{"location":"API/solvers/#RegularizedLeastSquares.POGM","page":"Solvers","title":"RegularizedLeastSquares.POGM","text":"POGM(A; AHA = A'*A, reg = L1Regularization(zero(real(eltype(AHA)))), normalizeReg = NoNormalization(), iterations = 50, verbose = false, rho = 0.95 / power_iterations(AHA), theta = 1, sigma_fac = 1, relTol = eps(real(eltype(AHA))), restart = :none)\nPOGM( ; AHA = ,     reg = L1Regularization(zero(real(eltype(AHA)))), normalizeReg = NoNormalization(), iterations = 50, verbose = false, rho = 0.95 / power_iterations(AHA), theta = 1, sigma_fac = 1, relTol = eps(real(eltype(AHA))), restart = :none)\n\nCreates a POGM object for the forward operator A or normal operator AHA. POGM has a 2x better worst-case bound than FISTA, but actual performance varies by application. It stores 3 extra intermediate variables the size of the image compared to FISTA. Only gradient restart scheme is implemented for now.\n\nReferences:\n\nA.B. Taylor, J.M. Hendrickx, F. Glineur,   \"Exact worst-case performance of first-order algorithms   for composite convex optimization,\" Arxiv:1512.07516, 2015,   SIAM J. Opt. 2017   [http://doi.org/10.1137/16m108104x]\nKim, D., & Fessler, J. A. (2018).   Adaptive Restart of the Optimized Gradient Method for Convex Optimization.   Journal of Optimization Theory and Applications, 178(1), 240–263.   [https://doi.org/10.1007/s10957-018-1287-4]\nRequired Arguments\nA                                                 - forward operator\nOR\nAHA                                               - normal operator (as a keyword argument)\nOptional Keyword Arguments\nAHA                                               - normal operator is optional if A is supplied\nreg::AbstractParameterizedRegularization          - regularization term\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\nrho::Real                                         - step size for gradient step; the default is 0.95 / max_eigenvalue as determined with power iterations.\ntheta::Real                                       - parameter for predictor-corrector step\nsigma_fac::Real                                   - parameter for decreasing γ-momentum ∈ [0,1]\nrelTol::Real                                      - tolerance for stopping criterion\niterations::Int                                   - maximum number of iterations\nrestart::Symbol                                   - :none, :gradient options for restarting\nverbose::Bool                                     - print residual in each iteration\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#SplitBregman","page":"Solvers","title":"SplitBregman","text":"","category":"section"},{"location":"API/solvers/","page":"Solvers","title":"Solvers","text":"RegularizedLeastSquares.SplitBregman","category":"page"},{"location":"API/solvers/#RegularizedLeastSquares.SplitBregman","page":"Solvers","title":"RegularizedLeastSquares.SplitBregman","text":"SplitBregman(A; AHA = A'*A, precon = Identity(), reg = L1Regularization(zero(real(eltype(AHA)))), regTrafo = opEye(eltype(AHA), size(AHA,1)), normalizeReg = NoNormalization(), rho = 1e-1, iterations = 10, iterationsInner = 10, iterationsCG = 10, absTol = eps(real(eltype(AHA))), relTol = eps(real(eltype(AHA))), tolInner = 1e-5, verbose = false)\nSplitBregman( ; AHA = ,     precon = Identity(), reg = L1Regularization(zero(real(eltype(AHA)))), regTrafo = opEye(eltype(AHA), size(AHA,1)), normalizeReg = NoNormalization(), rho = 1e-1, iterations = 10, iterationsInner = 10, iterationsCG = 10, absTol = eps(real(eltype(AHA))), relTol = eps(real(eltype(AHA))), tolInner = 1e-5, verbose = false)\n\nCreates a SplitBregman object for the forward operator A or normal operator AHA.\n\nRequired Arguments\n\nA                                                 - forward operator\n\nOR\n\nAHA                                               - normal operator (as a keyword argument)\n\nOptional Keyword Arguments\n\nAHA                                               - normal operator is optional if A is supplied\nprecon                                            - preconditionner for the internal CG algorithm\nreg::AbstractParameterizedRegularization          - regularization term; can also be a vector of regularization terms\nregTrafo                                          - transformation to a space in which reg is applied; if reg is a vector, regTrafo has to be a vector of the same length. Use opEye(eltype(AHA), size(AHA,1)) if no transformation is desired.\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\nrho::Real                                         - weights for condition on regularized variables; can also be a vector for multiple regularization terms\niterations::Int                              - maximum number of outer iterations. Set to 1 for unconstraint split Bregman (equivalent to ADMM)\niterationsInner::Int                              - maximum number of inner iterations\niterationsCG::Int                                 - maximum number of (inner) CG iterations\nabsTol::Real                                      - absolute tolerance for stopping criterion\nrelTol::Real                                      - relative tolerance for stopping criterion\ntolInner::Real                                    - relative tolerance for CG stopping criterion\nverbose::Bool                                     - print residual in each iteration\n\nThis algorithm solves the constraint problem (Eq. (4.7) in Tom Goldstein and Stanley Osher), i.e. ||R(x)||₁ such that ||Ax -b||₂² < σ². In order to solve the unconstraint problem (Eq. (4.8) in Tom Goldstein and Stanley Osher), i.e. ||Ax -b||₂² + λ ||R(x)||₁, you can either set iterations=1 or use ADMM instead, which is equivalent (iterations=1 in SplitBregman in implied in ADMM and the SplitBregman variable iterationsInner is simply called iterations in ADMM)\n\nLike ADMM, SplitBregman differs from ISTA-type algorithms in the sense that the proximal operation is applied separately from the transformation to the space in which the penalty is applied. This is reflected by the interface which has reg and regTrafo as separate arguments. E.g., for a TV penalty, you should NOT set reg=TVRegularization, but instead use reg=L1Regularization(λ), regTrafo=RegularizedLeastSquares.GradientOp(Float64; shape=(Nx,Ny,Nz)).\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#Miscellaneous","page":"Solvers","title":"Miscellaneous","text":"","category":"section"},{"location":"API/solvers/","page":"Solvers","title":"Solvers","text":"RegularizedLeastSquares.solverstate\nRegularizedLeastSquares.solversolution\nRegularizedLeastSquares.solverconvergence\nRegularizedLeastSquares.StoreSolutionCallback\nRegularizedLeastSquares.StoreConvergenceCallback\nRegularizedLeastSquares.CompareSolutionCallback\nRegularizedLeastSquares.linearSolverList\nRegularizedLeastSquares.createLinearSolver\nRegularizedLeastSquares.applicableSolverList\nRegularizedLeastSquares.isapplicable","category":"page"},{"location":"API/solvers/#RegularizedLeastSquares.solverstate","page":"Solvers","title":"RegularizedLeastSquares.solverstate","text":"solverstate(solver::AbstractLinearSolver)\n\nReturn the current state of the solver\n\n\n\n\n\n","category":"function"},{"location":"API/solvers/#RegularizedLeastSquares.solversolution","page":"Solvers","title":"RegularizedLeastSquares.solversolution","text":"solversolution(solver::AbstractLinearSolver)\n\nReturn the current solution of the solver\n\n\n\n\n\nsolversolution(state::AbstractSolverState)\n\nReturn the current solution of the solver's state\n\n\n\n\n\n","category":"function"},{"location":"API/solvers/#RegularizedLeastSquares.solverconvergence","page":"Solvers","title":"RegularizedLeastSquares.solverconvergence","text":"solverconvergence(solver::AbstractLinearSolver)\n\nReturn a named tuple of the solvers current convergence metrics\n\n\n\n\n\n","category":"function"},{"location":"API/solvers/#RegularizedLeastSquares.StoreSolutionCallback","page":"Solvers","title":"RegularizedLeastSquares.StoreSolutionCallback","text":"StoreSolutionCallback(T)\n\nCallback that accumlates the solvers solution per iteration. Results are stored in the solutions field.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.StoreConvergenceCallback","page":"Solvers","title":"RegularizedLeastSquares.StoreConvergenceCallback","text":"StoreConvergenceCallback()\n\nCallback that accumlates the solvers convergence metrics per iteration. Results are stored in the convMeas field.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.CompareSolutionCallback","page":"Solvers","title":"RegularizedLeastSquares.CompareSolutionCallback","text":"CompareSolutionCallback(ref, cmp)\n\nCallback that compares the solvers current solution with the given reference via cmp(ref, solution) per iteration. Results are stored in the results field.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.linearSolverList","page":"Solvers","title":"RegularizedLeastSquares.linearSolverList","text":"Return a list of all available linear solvers\n\n\n\n\n\n","category":"function"},{"location":"API/solvers/#RegularizedLeastSquares.createLinearSolver","page":"Solvers","title":"RegularizedLeastSquares.createLinearSolver","text":"createLinearSolver(solver::AbstractLinearSolver, A; kargs...)\n\nThis method creates a solver. The supported solvers are methods typically used for solving regularized linear systems. All solvers return an approximate solution to Ax = b.\n\nTODO: give a hint what solvers are available\n\n\n\n\n\n","category":"function"},{"location":"API/solvers/#RegularizedLeastSquares.applicableSolverList","page":"Solvers","title":"RegularizedLeastSquares.applicableSolverList","text":"applicable(args...)\n\nlist all solvers that are applicable to the given arguments. Arguments are the same as for isapplicable without the solver type.\n\nSee also isapplicable, linearSolverList.\n\n\n\n\n\n","category":"function"},{"location":"API/solvers/#RegularizedLeastSquares.isapplicable","page":"Solvers","title":"RegularizedLeastSquares.isapplicable","text":"isapplicable(solverType::Type{<:AbstractLinearSolver}, A, x, reg)\n\nreturn true if a solver of type solverType is applicable to system matrix A, data x and regularization terms reg.\n\n\n\n\n\n","category":"function"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"EditURL = \"../../literate/howto/gpu_acceleration.jl\"","category":"page"},{"location":"generated/howto/gpu_acceleration/#GPU-Acceleration","page":"GPU Acceleration","title":"GPU Acceleration","text":"","category":"section"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"RegularizedLeastSquares.jl supports generic GPU acceleration. This means that the user can use any GPU array type that supports the GPUArrays interface. This includes CUDA.jl, AMDGPU.jl, and Metal.jl. In this example we will use the package JLArrays.jl which provides a reference implementation for GPUArrays, that can runs on CPUs.","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"using JLArrays\ngpu = JLArray;\nnothing #hide","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"To use the following examples on an actual GPU, load the appropraite package replace JLArray with the respective GPU array type, for example:","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"using CUDA\ngpu = CuArray","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"At first we will look at an example of dense GPU arrays.","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"using RegularizedLeastSquares\nA = gpu(rand(Float32, 32, 16))\nx = gpu(rand(Float32, 16))\nb = A*x;\nnothing #hide","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"Solvers adapt their states based on the type of the given measurement vector. This means that the solver will automatically switch to GPU acceleration if a GPU array is passed as the measurement vector.","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"solver = createLinearSolver(CGNR, A; reg = L2Regularization(0.0001), iterations=32);\nx_approx = solve!(solver, b)","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"This adaption does not include the operator. So if we want to compare with CPU result, we need to construct a new solver with a CPU operator.","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"solver = createLinearSolver(CGNR, Array(A); reg = L2Regularization(0.0001), iterations=32);\nx_cpu = solve!(solver, Array(b))\nisapprox(Array(x_approx), x_cpu)","category":"page"},{"location":"generated/howto/gpu_acceleration/#Matrix-Free-Operators","page":"GPU Acceleration","title":"Matrix-Free Operators","text":"","category":"section"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"A special case is the usage of matrix-free operators. Since these operators do not have a concrete matrix representation, their GPU support depends on their implementation. Since not all multiplications within a solver approximation are in-place, the operator also needs to support the * operation and construct an appropriate result vector. For matrix-free operators based on LinearOperators.jl, this can be achieved by implementing the LinearOperators.storage_type method.","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"In the following, we will take another look at the CS example and execute it on the GPU. Note that for the JLArray example we chose a small phantom, since the JLArray implementation is not optimized for performance:","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"using ImagePhantoms, ImageGeoms\nN = 32\nimage = shepp_logan(N, SheppLoganToft())\n\nusing Random, LinearOperatorCollection\nrandomIndices = shuffle(eachindex(image))\nsampledIndices = sort(randomIndices[1:div(end, 3)]);\nnothing #hide","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"To construct the operator, we need to convert the indices to a GPU array. We also need to specify the correct storage type. In both LinearOperators.jl and LinearOperatorCollection.jl this is done with the S keyword argument.","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"gpu_indices = gpu(sampledIndices)\nA = SamplingOp(eltype(image), pattern = gpu_indices, shape = size(image), S = typeof(b));\nnothing #hide","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"Let's inspect the storage type of the operator:","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"using LinearOperatorCollection.LinearOperators\nLinearOperators.storage_type(A)","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"Afterwards we can use the operator as usual:","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"b = A*vec(gpu(image));\nnothing #hide","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"And use it in the solver:","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"using RegularizedLeastSquares\nreg = TVRegularization(0.01; shape=size(image))\nsolver = createLinearSolver(FISTA, A; reg=reg, iterations=20)\nimg_approx = solve!(solver,b);\nnothing #hide","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"To visualize the reconstructed image, we need to reshape the result vector to the correct shape and convert it to a CPU array:","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"img_approx = reshape(Array(img_approx),size(image))","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"We will again use CairoMakie for visualization:","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"using CairoMakie\nfunction plot_image(figPos, img; title = \"\", width = 150, height = 150)\n  ax = CairoMakie.Axis(figPos; yreversed=true, title, width, height)\n  hidedecorations!(ax)\n  heatmap!(ax, img)\nend\nfig = Figure()\nplot_image(fig[1,1], image, title = \"Image\")\nsamplingMask = fill(false, size(image))\nsamplingMask[sampledIndices] .= true\nplot_image(fig[1,2], image .* samplingMask, title = \"Sampled Image\")\nplot_image(fig[1,3], img_approx, title = \"Reconstructed Image\")\nresize_to_layout!(fig)\nfig","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"","category":"page"},{"location":"generated/howto/gpu_acceleration/","page":"GPU Acceleration","title":"GPU Acceleration","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"EditURL = \"../../literate/examples/compressed_sensing.jl\"","category":"page"},{"location":"generated/examples/compressed_sensing/#Compressed-Sensing-Example","page":"Compressed Sensing","title":"Compressed Sensing Example","text":"","category":"section"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"In this example we will go through a simple example from the field of Compressed Sensing. In addtion to RegularizedLeastSquares.jl, we will need the packages LinearOperatorCollection.jl, ImagePhantoms.jl, ImageGeoms.jl and Random.jl, as well as CairoMakie.jl for visualization. We can install them the same way we did RegularizedLeastSquares.jl.","category":"page"},{"location":"generated/examples/compressed_sensing/#Preparing-the-Inverse-Problem","page":"Compressed Sensing","title":"Preparing the Inverse Problem","text":"","category":"section"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"To get started, let us generate a simple phantom using the ImagePhantoms and ImageGeom packages:","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"using ImagePhantoms, ImageGeoms\nN = 256\nimage = shepp_logan(N, SheppLoganToft())\nsize(image)","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"This produces a 256x256 image of a Shepp-Logan phantom.","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"In this example, we consider a problem in which we randomly sample a third of the pixels in the image. Such a problem and the corresponding measurement can be constructed with the packages LinearOperatorCollection and Random:","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"We first randomly shuffle the indices of the image and then select the first third of the indices to sample.","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"using Random, LinearOperatorCollection\nrandomIndices = shuffle(eachindex(image))\nsampledIndices = sort(randomIndices[1:div(end, 3)])","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"Afterwards we build a sampling operator which samples the image at the selected indices.","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"A = SamplingOp(eltype(image), pattern = sampledIndices , shape = size(image));\nnothing #hide","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"Then we apply the sampling operator to the vectorized image to obtain the sampled measurement vector","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"b = A*vec(image);\nnothing #hide","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"To visualize our image we can use CairoMakie:","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"using CairoMakie\nfunction plot_image(figPos, img; title = \"\", width = 150, height = 150)\n  ax = CairoMakie.Axis(figPos; yreversed=true, title, width, height)\n  hidedecorations!(ax)\n  heatmap!(ax, img)\nend\nfig = Figure()\nplot_image(fig[1,1], image, title = \"Image\")\nsamplingMask = fill(false, size(image))\nsamplingMask[sampledIndices] .= true\nplot_image(fig[1,2], image .* samplingMask, title = \"Sampled Image\")\nresize_to_layout!(fig)\nfig","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"As we can see in the right image, only a third of the pixels are sampled. The goal of the inverse problem is to recover the original image from this measurement vector.","category":"page"},{"location":"generated/examples/compressed_sensing/#Solving-the-Inverse-Problem","page":"Compressed Sensing","title":"Solving the Inverse Problem","text":"","category":"section"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"To recover the image from the measurement vector, we solve the TV-regularized least squares problem:","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"beginequation\n  undersetmathbfxargmin frac12vertvert mathbfAmathbfx-mathbfb vertvert_2^2 + lambdavertvertmathbfxvertvert_textTV \nendequation","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"For this purpose we build a TV regularizer with regularization parameter λ=001","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"using RegularizedLeastSquares\nreg = TVRegularization(0.01; shape=size(image));\nnothing #hide","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"We will use the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) to solve our inverse problem. Thus, we build the corresponding solver","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"solver = createLinearSolver(FISTA, A; reg=reg, iterations=20);\nnothing #hide","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"and apply it to our measurement vector","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"img_approx = solve!(solver,b)","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"To visualize the reconstructed image, we need to reshape the result vector to the correct shape. Afterwards we can use CairoMakie again:","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"img_approx = reshape(img_approx,size(image));\nplot_image(fig[1,3], img_approx, title = \"Reconstructed Image\")\nresize_to_layout!(fig)\nfig","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"","category":"page"},{"location":"generated/examples/compressed_sensing/","page":"Compressed Sensing","title":"Compressed Sensing","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"EditURL = \"../../literate/examples/getting_started.jl\"","category":"page"},{"location":"generated/examples/getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"In this example we will go through a simple random inverse problem to get familiar with RegularizedLeastSquares.jl.","category":"page"},{"location":"generated/examples/getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"To install RegularizedLeastSquares.jl, you can use the Julia package manager. Open a Julia REPL and run the following command:","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"using Pkg\nPkg.add(\"RegularizedLeastSquares\")","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"This will download and install the RegularizedLeastSquares.jl package and its dependencies. To install a different version, please consult the Pkg documentation.","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"Once the installation is complete, you can import the package with the using keyword:","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"using RegularizedLeastSquares","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"RegularizedLeastSquares aims to solve inverse problems of the form:","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"beginequation\n  undersetmathbfxargmin frac12vertvert mathbfAmathbfx-mathbfb vertvert_2^2 + mathbfR(x) \nendequation","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"where mathbfA is a linear operator, mathbfb is the measurement vector, and mathbfR(x) is an (optional) regularization term. The goal is to retrieve an approximation of the unknown vector mathbfx. In this first exampel we will just work with simple random arrays. For more advanced examples, please refer to the examples.","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"A = rand(32, 16)\nx = rand(16)\nb = A*x;\nnothing #hide","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"To approximate x from b, we can use the Conjugate Gradient Normal Residual (CGNR) algorithm. We first build the corresponding solver:","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"solver = createLinearSolver(CGNR, A; iterations=32);\nnothing #hide","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"and apply it to our measurement vector","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"x_approx = solve!(solver, b)\nisapprox(x, x_approx, rtol = 0.001)","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"Usually the inverse problems are ill-posed and require regularization. RegularizedLeastSquares.jl provides a variety of regularization terms. The CGNR algorithm can solve optimzation problems of the form:","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"beginequation\n  undersetmathbfxargmin frac12vertvert mathbfAmathbfx-mathbfb vertvert_2^2 + lambdavertvertmathbfxvertvert^2_2 \nendequation","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"The corresponding solver can be built with the l^2_2-regularization term:","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"solver = createLinearSolver(CGNR, A; reg = L2Regularization(0.0001), iterations=32);\nx_approx = solve!(solver, b)\nisapprox(x, x_approx, rtol = 0.001)","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"","category":"page"},{"location":"generated/examples/getting_started/","page":"Getting Started","title":"Getting Started","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"EditURL = \"../../literate/explanations/regularization.jl\"","category":"page"},{"location":"generated/explanations/regularization/#Regularization","page":"Regularization","title":"Regularization","text":"","category":"section"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"When formulating inverse problems, a regulariser is formulated as an additional cost function which has to be minimised. Many algorithms deal with a regularizers g, by computing the proximal map:","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"beginequation\n  prox_g (mathbfx) = undersetmathbfuargmin frac12vertvert mathbfu-mathbfx vert vert^2 + g(mathbfx)\nendequation","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"For many regularizers, the proximal map can be computed efficiently in a closed form.","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"In order to implement these proximal mappings, RegularizedLeastSquares.jl defines the following type hierarchy:","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"abstract type AbstractRegularization\nprox!(reg::AbstractRegularization, x)\nnorm(reg::AbstractRegularization, x)","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"Here prox!(reg, x) is an in-place function which computes the proximal map on the input-vector x. The function norm computes the value of the corresponding term in the inverse problem. RegularizedLeastSquares.jl provides AbstractParameterizedRegularization and AbstractProjectionRegularization as core regularization types.","category":"page"},{"location":"generated/explanations/regularization/#Parameterized-Regularization-Terms","page":"Regularization","title":"Parameterized Regularization Terms","text":"","category":"section"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"This group of regularization terms features a regularization parameter λ that is used during the prox! and normcomputations. Examples of this regulariztion group are L1, L2 or LLR (locally low rank) regularization terms.","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"These terms are constructed by supplying a λ and optionally term specific keyword arguments:","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"using RegularizedLeastSquares\nl2 = L2Regularization(0.3)","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"Parameterized regularization terms implement:","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"prox!(reg::AbstractParameterizedRegularization, x, λ)\nnorm(reg::AbstractParameterizedRegularization, x, λ)","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"where λ by default is filled with the value used during construction.","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"Invoking λ on a parameterized term retrieves its regularization parameter. This can be used in a solver to scale and overwrite the parameter as follows:","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"prox!(l2, [1.0])","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"param = λ(l2)\nprox!(l2, [1.0], param*0.2)","category":"page"},{"location":"generated/explanations/regularization/#Projection-Regularization-Terms","page":"Regularization","title":"Projection Regularization Terms","text":"","category":"section"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"This group of regularization terms implement projections, such as a positivity constraint or a projection with a given convex projection function. These are essentially proximal maps where g(mathbfx) is the indicator function of a convex set.","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"positive = PositiveRegularization()\nprox!(positive, [2.0, -0.2])","category":"page"},{"location":"generated/explanations/regularization/#Nested-Regularization-Terms","page":"Regularization","title":"Nested Regularization Terms","text":"","category":"section"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"Nested regularization terms are terms that act as decorators to the core regularization terms. These terms can be nested around other terms and add functionality to a regularization term, such as scaling λ based on the provided operator or applying a transform, such as the Wavelet, to x. As an example, we can nest a L1 regularization term around a Wavelet operator.","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"First we generate an image and apply a wavelet operator to it:","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"using Wavelets, LinearOperatorCollection, ImagePhantoms, ImageGeoms\nN = 256\nimage = shepp_logan(N, SheppLoganToft())\nwop = WaveletOp(Float32, shape = size(image))\n\nwavelet_image = reshape(wop*vec(image), size(image))\nwavelet_image = log.(abs.(wavelet_image) .+ 0.01)","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"We will use CairoMakie for visualization:","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"using CairoMakie\nfunction plot_image(figPos, img; title = \"\", width = 150, height = 150)\n  ax = CairoMakie.Axis(figPos; yreversed=true, title, width, height)\n  hidedecorations!(ax)\n  heatmap!(ax, img)\nend\nfig = Figure()\nplot_image(fig[1,1], image, title = \"Image\")\nplot_image(fig[1,2], wavelet_image, title = \"Wavelet Image\")\nresize_to_layout!(fig)\nfig","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"To apply soft-thresholding in the wavelet domain, we can nest a L1 regularization term around the Wavelet operator:","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"core = L1Regularization(0.1)\nreg = TransformedRegularization(core, wop);\nnothing #hide","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"We can then apply the proximal map to the image or the image in the wavelet domain:","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"img_prox_image = prox!(core, copy(vec(image)));\nimg_prox_wavelet = prox!(reg, copy(vec(image)));\nnothing #hide","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"We can visualize the result:","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"img_prox_image = reshape(img_prox_image, size(image))\nimg_prox_wavelet = reshape(img_prox_wavelet, size(image))\nplot_image(fig[1,3], img_prox_image, title = \"Reg. Image Domain\")\nplot_image(fig[1,4], img_prox_wavelet, title = \"Reg. Wavelet Domain\")\nresize_to_layout!(fig)\nfig","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"Generally, regularization terms can be nested arbitrarly deep, adding new functionality with each layer. Each nested regularization term can return its inner regularization term. Furthermore, all regularization terms implement the iteration interface to iterate over the nesting. The innermost regularization term of a nested term must be a core regularization term and it can be returned by the sink function:","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"RegularizedLeastSquares.innerreg(reg) == core","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"sink(reg) == core","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"foreach(r -> println(nameof(typeof(r))), reg)","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"","category":"page"},{"location":"generated/explanations/regularization/","page":"Regularization","title":"Regularization","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#RegularizedLeastSquares.jl","page":"Home","title":"RegularizedLeastSquares.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Solvers for Linear Inverse Problems using Regularization Techniques","category":"page"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RegularizedLeastSquares.jl is a Julia package for solving large linear systems using various types of algorithms. Ill-conditioned problems arise in many areas of practical interest. Regularisation techniques and nonlinear problem formulations are often used to solve these problems. This package provides implementations for a variety of solvers used in areas such as MPI and MRI. In particular, this package serves as the optimization backend of the Julia packages MPIReco.jl and MRIReco.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The implemented methods range from the l^2_2-regularized CGNR method to more general optimizers such as the Alternating Direction of Multipliers Method (ADMM) or the Split-Bregman method.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For convenience, implementations of popular regularizers, such as l_1-regularization and TV regularization, are provided. On the other hand, hand-crafted regularizers can be used quite easily.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Depending on the problem, it becomes unfeasible to store the full system matrix at hand. For this purpose, RegularizedLeastSquares.jl allows for the use of matrix-free operators. Such operators can be realized using the interface provided by the package LinearOperators.jl. Other interfaces can be used as well, as long as the product *(A,x) and the adjoint adjoint(A) are provided. A number of common matrix-free operators are provided by the package LinearOperatorColection.jl.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Variety of optimization algorithms optimized for least squares problems\nSupport for matrix-free operators\nGPU support","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See Getting Started for an introduction to using the package","category":"page"},{"location":"#See-also","page":"Home","title":"See also","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Packages:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ProximalAlgorithms.jl\nProximalOperators.jl\nKrylov.jl\nRegularizedOptimization.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"Organizations:","category":"page"},{"location":"","page":"Home","title":"Home","text":"JuliaNLSolvers\nJuliaSmoothOptimizers\nJuliaFirstOrder","category":"page"}]
}
