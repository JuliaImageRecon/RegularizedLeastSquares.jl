var documenterSearchIndex = {"docs":
[{"location":"generated/examples/computed_tomography/#Computed-Tomography-Example","page":"Computed Tomography","title":"Computed Tomography Example","text":"In this example we will go through a simple example from the field of Computed Tomography. In addtion to RegularizedLeastSquares.jl, we will need the packages LinearOperatorCollection.jl, ImagePhantoms.jl, ImageGeoms.jl and RadonKA.jl, as well as CairoMakie.jl for visualization. We can install them the same way we did RegularizedLeastSquares.jl.\n\nRadonKA is a package for the computation of the Radon transform and its adjoint. It is implemented with KernelAbstractions.jl and supports GPU acceleration. See the GPU acceleration how-to for more information.","category":"section"},{"location":"generated/examples/computed_tomography/#Preparing-the-Inverse-Problem","page":"Computed Tomography","title":"Preparing the Inverse Problem","text":"To get started, let us generate a simple phantom using the ImagePhantoms and ImageGeom packages:\n\nusing ImagePhantoms, ImageGeoms\nN = 256\nimage = shepp_logan(N, SheppLoganToft())\nsize(image)\n\nThis produces a 256x256 image of a Shepp-Logan phantom.\n\nusing RadonKA, LinearOperatorCollection\nangles = collect(range(0, π, 256))\nsinogram = Array(RadonKA.radon(image, angles))\n\nAfterwards we build a Radon operator implementing both the forward and adjoint Radon transform\n\nA = RadonOp(eltype(image); angles, shape = size(image));\nnothing #hide\n\nTo visualize our image we can use CairoMakie:\n\nusing CairoMakie\nfunction plot_image(figPos, img; title = \"\", width = 150, height = 150)\n  ax = CairoMakie.Axis(figPos[1, 1]; yreversed=true, title, width, height)\n  hidedecorations!(ax)\n  hm = heatmap!(ax, img)\n  Colorbar(figPos[2, 1], hm, vertical = false, flipaxis = false)\nend\nfig = Figure()\nplot_image(fig[1,1], image, title = \"Image\")\nplot_image(fig[1,2], sinogram, title = \"Sinogram\")\nplot_image(fig[1,3], backproject(sinogram, angles), title = \"Backprojection\")\nresize_to_layout!(fig)\nfig\n\nIn the figure we can see our original image, the sinogram, and the backprojection of the sinogram. The goal of the inverse problem is to recover the original image from the sinogram.","category":"section"},{"location":"generated/examples/computed_tomography/#Solving-the-Inverse-Problem","page":"Computed Tomography","title":"Solving the Inverse Problem","text":"To recover the image from the measurement vector, we solve the l^2_2-regularized least squares problem\n\nbeginequation\n  undersetmathbfxargmin frac12vertvert mathbfAmathbfx-mathbfb vertvert_2^2 + lambdavertvertmathbfxvertvert^2_2 \nendequation\n\nFor this purpose we build a l^2_2 with regularization parameter λ=0001\n\nusing RegularizedLeastSquares\nreg = L2Regularization(0.001);\nnothing #hide\n\nTo solve this inverse problem, the Conjugate Gradient Normal Residual (CGNR) algorithm can be used. This solver is based on the normal operator of the Radon operator and uses both the forward and adjoint Radon transform internally. We now build the corresponding solver\n\nsolver = createLinearSolver(CGNR, A; reg=reg, iterations=20);\nnothing #hide\n\nand apply it to our measurement vector\n\nimg_approx = solve!(solver, vec(sinogram))\n\nTo visualize the reconstructed image, we need to reshape the result vector to the correct shape. Afterwards we can use CairoMakie again:\n\nimg_approx = reshape(img_approx,size(image));\nplot_image(fig[1,4], img_approx, title = \"Reconstructed Image\")\nresize_to_layout!(fig)\nfig\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/howto/efficient_kaczmarz/#Efficient-Kaczmarz","page":"Efficient Kaczmarz","title":"Efficient Kaczmarz","text":"Unlike many of the other solvers provided by RegularizedLeastSquares.jl, the Kaczmarz method does not utilize a matrix-vector product with the operator mathbfA nor the normal operator mathbfA*A. Instead, it uses the rows of mathbfA to update the solution iteratively. Efficient Kaczmarz implementation therefore require very efficient dot products with the rows of mathbfA. In RegularizedLeastSquares.jl, this is achieved with the dot_with_matrix_row function.\n\nusing RegularizedLeastSquares\nA = randn(256, 256)\nx = randn(256)\nb = A*x;\nnothing #hide\n\nThe dot_with_matrix_row function calculates the dot product between a row of A and the current approximate solution of x:\n\nrow = 1\nisapprox(RegularizedLeastSquares.dot_with_matrix_row(A, x, row), sum(A[row, :] .* x))\n\nSince in Julia, dense arrays are stored in column-major order, such a row-based operation is quite inefficient. A workaround is to transpose the matrix then pass it to a Kaczmarz solver.\n\nAt = collect(transpose(A))\nA_eff = transpose(At)\n\nNote that the transpose function can return a lazy transpose object, so we first collect the transpose into a dense matrix. Then we transpose it again to get the efficient representation of the matrix.\n\nWe can compare the performance using the BenchmarkTools.jl package. First for the original matrix:\n\nusing BenchmarkTools\nsolver = createLinearSolver(Kaczmarz, A; reg = L2Regularization(0.0001), iterations=100)\n@benchmark solve!(solver, b) samples = 100\n\nAnd then for the efficient matrix:\n\nsolver_eff = createLinearSolver(Kaczmarz, A_eff; reg = L2Regularization(0.0001), iterations=100)\n@benchmark solve!(solver_eff, b) samples = 100\n\nWe can also combine the efficient matrix with a weighting matrix, as is shown in the Weighting example.\n\nCustom operators need to implement the dot_with_matrix_row function to be used with the Kaczmarz solver. Ideally, such an implementation is allocation free.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/howto/multi_threading/#Multi-Threading","page":"Multi-Threading","title":"Multi-Threading","text":"There are different ways multi-threading can be used with RegularizedLeastSquares.jl. To use multi-threading in Julia, one needs to start their session with multi-threads, see the Julia documentation for more information.","category":"section"},{"location":"generated/howto/multi_threading/#Solver-Based-Multi-Threading","page":"Multi-Threading","title":"Solver Based Multi-Threading","text":"This type of multi-threading is transparent to the solver and is applicable if the total solution is composed of individual solutions that can be solved in parallel. In particular, this approach also allows for using solvers with different parameters, such as their operator or regularization parameters.\n\nusing RegularizedLeastSquares\nAs = [rand(32, 16) for _ in 1:4]\nxs = [rand(16) for _ in 1:4]\nbs = [A*x for (A, x) in zip(As, xs)]\n\nxs_approx = similar(xs)\nThreads.@threads for i in 1:4\n  solver = createLinearSolver(CGNR, As[i]; iterations=32)\n  xs_approx[i] = solve!(solver, bs[i])\nend","category":"section"},{"location":"generated/howto/multi_threading/#Operator-Based-Multi-Threading","page":"Multi-Threading","title":"Operator Based Multi-Threading","text":"This type of multi-threading involves linear operators or proximal maps that can be implemnted in parallel. Examples of this include the proximal map of the TV regularization term, which is based on the multi-threaded GradientOp from LinearOperatorCollection. GPU acceleration also falls under this approach, see GPU Acceleration for more information.","category":"section"},{"location":"generated/howto/multi_threading/#Measurement-Based-Multi-Threading","page":"Multi-Threading","title":"Measurement Based Multi-Threading","text":"This level of multi-threading applies the same solver (and its parameters) to multiple measurement vectors or rather a measurement matrix B. This is useful in the case of multiple measurements that can be solved in parallel and can reuse the same solver. This approach is not applicable if the operator is stateful.\n\nTo use this approach we first build a measurement matrix B and a corresponding solver:\n\nA = first(As)\nB = mapreduce(x -> A*x, hcat, xs)\nsolver = createLinearSolver(CGNR, A; iterations=32)\n\nWe can then simply pass the measurement matrix to the solver. The result will be the same as if we passed each colument of B seperately:\n\nx_approx = solve!(solver, B)\nsize(x_approx)\n\nThe previous solve! call was still executed sequentially. To execute it in parallel, we have to specify a multi-threaded scheduler as a keyword-argument of the solve! call. RegularizedLeastSquares.jl provides a MultiThreadingState scheduler that can be used for this purpose. This scheduler is based on the Threads.@threads macro:\n\nx_multi = solve!(solver, B; scheduler = MultiThreadingState)\nx_approx == x_multi","category":"section"},{"location":"generated/howto/multi_threading/#Custom-Scheduling","page":"Multi-Threading","title":"Custom Scheduling","text":"It is possible to implement custom scheduling. The following code shows how to implement this for the Threads.@spawn macro. Usually one this to implement multi-threading with a package such as FLoop.jl or ThreadPools.jl for thread pinning:\n\nSince most solver have conv. criteria, they can finish at different iteration numbers, which we track this information with flags.\n\n mutable struct SpawnState{S, ST <: AbstractSolverState{S}} <: RegularizedLeastSquares.AbstractMatrixSolverState{S}\n   states::Vector{ST}\n   active::Vector{Bool}\n   SpawnState(states::Vector{ST}) where {S, ST <: AbstractSolverState{S}} = new{S, ST}(states, fill(true, length(states)))\n end\n\nTo hook into the existing init! code we only have to supply a method that gets a copyable \"vector\" state. This will invoke our SpawnState constructor with copies of the given state.\n\n prepareMultiStates(solver::AbstractLinearSolver, state::SpawnState, b::AbstractMatrix) = prepareMultiStates(solver, first(state.states), b);\nnothing #hide\n\nWe specialise the iterate function which is called with the idx of still active states\n\nfunction Base.iterate(solver::AbstractLinearSolver, state::SpawnState, activeIdx)\n  @sync Threads.@spawn for i in activeIdx\n    res = iterate(solver, state.states[i])\n    if isnothing(res)\n      state.active[i] = false\n    end\n  end\n  return state.active, state\nend\n\nNow we can simply use the SpawnState scheduler in the solve! call:\n\nx_custom = solve!(solver, B; scheduler = SpawnState)\nx_approx == x_multi\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"solvers/#Solvers","page":"Solvers","title":"Solvers","text":"RegularizedLeastSquares.jl provides a variety of solvers, which are used in fields such as MPI and MRI. The following gives an overview of implemented solvers and roughly which regularization terms are applicable for them:\n\nSolver Regularization\nKaczmarz (Also called Algebraic reconstruction technique) l_2^2 and one additional term\nConjugate Gradients Normal Residual method (CGNR) l_2^2\nFast Iterative Shrinkage Thresholding Algorithm (FISTA) Any one term\nOptimal Iterative Shrinkage Thresholding Algorithm (OptISTA) Any one term\nProximal Optimized Gradient Method (POGM) Any one term\nAlternating Direction of Multipliers Method (ADMM) Several terms\nSplitBregman Several terms\nDirectSolver l_2^2\nPseudoInverse l_2^2\n\nIt is also possible to provide custom terms by implementing a proximal mapping. Generally, these algorithms are correct for regularization terms that are convex and possibly non-smooth. See Regularization, for more information on regularization terms. \n\nFor convenience reasons, all solvers accept projection regularization terms, such as a constraint to the positive numbers. Depending on the solver, such a term is either considered during every iteration or just in the last iteration. \n\nA list of all available solvers can be returned by the linearSolverList function.","category":"section"},{"location":"solvers/#Solver-Construction","page":"Solvers","title":"Solver Construction","text":"To create a solver, one can invoke the method createLinearSolver as in\n\nsolver = createLinearSolver(CGNR, A; reg=reg, kwargs...)\n\nHere A denotes the operator and reg are the Regularization terms to be used by the solver. All further solver parameters can be passed as keyword arguments and are solver specific. To make things more compact, it can be usefull to collect all parameters in a Dict{Symbol,Any}. In this way, the code snippet above can be written as\n\nparams=Dict{Symbol,Any}()\nparams[:reg] = ...\n...\n\nsolver = createLinearSolver(CGNR, A; params...)\n\nThis notation can be convenient when a large number of parameters are set manually.\n\nIt is also possible to construct a solver directly with its specific keyword arguments:\n\nsolver = CGNR(A, reg = reg, ...)","category":"section"},{"location":"solvers/#Solver-Usage","page":"Solvers","title":"Solver Usage","text":"Once constructed, a solver can be used to approximate a solution to a given measurement vector:\n\nx_approx = solve!(solver, b; kwargs...)\n\nThe keyword arguments can be used to supply an inital solution x0, one or more callbacks to interact and monitor the solvers state and more. See the How-To and the API for more information.\n\nIt is also possible to explicitly invoke the solvers iterations using Julias iterate interface:\n\ninit!(solver, b; kwargs...)\nfor (iteration, x_approx) in enumerate(solver)\n    println(\"Iteration $iteration\")\nend","category":"section"},{"location":"solvers/#Solver-Internals","page":"Solvers","title":"Solver Internals","text":"The solvers are organized in a type-hierarchy and inherit from:\n\nabstract type AbstractLinearSolver\n\nThe type hierarchy is further differentiated into solver categories such as AbstractRowAtionSolver, AbstractPrimalDualSolver or AbstractProximalGradientSolver. \n\nThe fields of a solver can be divided into two groups. The first group are intended to be immutable fields that do not change during iterations, the second group are mutable fields that do change. Examples of the first group are the operator itself and examples of the second group are the current solution or the number of the current iteration.\n\nThe second group is usually encapsulated in its own state struct:\n\nmutable struct Solver{matT, ...}\n  A::matT\n  # Other \"static\" fields\n  state::AbstractSolverState{<:Solver}\nend\n\nmutable struct SolverState{T, tempT} <: AbstractSolverState{Solver}\n  x::tempT\n  rho::T\n  # ...\n  iteration::Int64\nend\n\nStates are subtypes of the parametric AbstractSolverState{S} type. The state fields of solvers can be exchanged with different state belonging to the correct solver S. This means that the states can be used to realize custom variants of an existing solver:\n\nmutable struct VariantState{T, tempT} <: AbstractSolverState{Solver}\n  x::tempT\n  other::tempT\n  # ...\n  iteration::Int64\nend\n\nSolverVariant(A; kwargs...) = Solver(A, VariantState(kwargs...))\n\nfunction iterate(solver::Solver, state::VarianteState)\n  # Custom iteration\nend","category":"section"},{"location":"generated/howto/weighting/#Weighting","page":"Weighting","title":"Weighting","text":"Often time one wants to solve a weighted least squares problem of the form:\n\nbeginequation\n  undersetmathbfxargmin frac12vertvert mathbfAmathbfx-mathbfb vertvert^2_mathbfW + mathbfR(x) \nendequation\n\nwhere mathbfW is a symmetric, positive weighting matrix and vertvertmathbfyvertvert^2_mathbfW denotes the weighted Euclidean norm. An example of such a weighting matrix is a noise whitening matrix. Another example could be a scaling of the matrix rows by the reciprocal of their row energy.\n\nIn the following, we will solve a weighted least squares problem of the form:\n\nbeginequation\n  undersetmathbfxargmin frac12vertvert mathbfAmathbfx-mathbfb vertvert_mathbfW^2 + lambdavertvertmathbfxvertvert^2_2 \nendequation\n\nusing RegularizedLeastSquares, LinearOperatorCollection, LinearAlgebra\nA = rand(32, 16)\nx = rand(16)\nb = A*x;\nnothing #hide\n\nAs a weighting matrix, we will use the reciprocal of the row energy of the matrix A.\n\nweights = map(row -> 1/rownorm²(A, row), 1:size(A, 1));\nnothing #hide\n\nFirst, let us solve the problem with matrices we manually weighted.\n\nWA = diagm(weights) * A\nsolver = createLinearSolver(Kaczmarz, WA; reg = L2Regularization(0.0001), iterations=10)\nx_approx = solve!(solver, weights .* b);\nnothing #hide\n\nThe operator A is not always a dense matrix and the product between the operator and the weighting matrix is not always efficient or possible. The package LinearOperatorCollection.jl provides a matrix-free implementation of a diagonal weighting matrix, as well as a matrix free product between two matrices. This weighted operator has efficient implementations of the normal operator and also for the row-action operations of the Kaczmarz solver.\n\nW = WeightingOp(weights)\nP = ProdOp(W, A)\nsolver = createLinearSolver(Kaczmarz, P; reg = L2Regularization(0.0001), iterations=10)\nx_approx2 = solve!(solver, W * b)\nisapprox(x_approx, x_approx2)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"API/regularization/#API-for-Regularizers","page":"Regularization Terms","title":"API for Regularizers","text":"This page contains documentation of the public API of the RegularizedLeastSquares. In the Julia REPL one can access this documentation by entering the help mode with ?","category":"section"},{"location":"API/regularization/#Projection-Regularization","page":"Regularization Terms","title":"Projection Regularization","text":"","category":"section"},{"location":"API/regularization/#Nested-Regularization","page":"Regularization Terms","title":"Nested Regularization","text":"","category":"section"},{"location":"API/regularization/#Scaled-Regularization","page":"Regularization Terms","title":"Scaled Regularization","text":"","category":"section"},{"location":"API/regularization/#Misc.-Nested-Regularization","page":"Regularization Terms","title":"Misc. Nested Regularization","text":"","category":"section"},{"location":"API/regularization/#Miscellaneous-Functions","page":"Regularization Terms","title":"Miscellaneous Functions","text":"","category":"section"},{"location":"API/regularization/#RegularizedLeastSquares.L1Regularization","page":"Regularization Terms","title":"RegularizedLeastSquares.L1Regularization","text":"L1Regularization\n\nRegularization term implementing the proximal map for the Lasso problem.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.L2Regularization","page":"Regularization Terms","title":"RegularizedLeastSquares.L2Regularization","text":"L2Regularization\n\nRegularization term implementing the proximal map for Tikhonov regularization.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.L21Regularization","page":"Regularization Terms","title":"RegularizedLeastSquares.L21Regularization","text":"L21Regularization\n\nRegularization term implementing the proximal map for group-soft-thresholding.\n\nArguments\n\nλ                  - regularization paramter\n\nKeywords\n\nslices=1           - number of elements per group\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.LLRRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.LLRRegularization","text":"LLRRegularization\n\nRegularization term implementing the proximal map for locally low rank (LLR) regularization using singular-value-thresholding. Computation is always performed on the CPU. If the input is a GPU array, it is temporarily moved to the CPU.\n\nArguments\n\nλ                  - regularization paramter\n\nKeywords\n\nshape::Tuple{Int}            - dimensions of the image\nblockSize::Tuple{Int}=(2,2)  - size of patches to perform singular value thresholding on\nrandshift::Bool=true         - randomly shifts the patches to ensure translation invariance\nfullyOverlapping::Bool=false - choose between fully overlapping block or non-overlapping blocks\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.NuclearRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.NuclearRegularization","text":"NuclearRegularization\n\nRegularization term implementing the proximal map for singular value soft-thresholding.\n\nArguments:\n\nλ           - regularization paramter\n\nKeywords\n\nsvtShape::NTuple  - size of the underlying matrix\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.TVRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.TVRegularization","text":"TVRegularization\n\nRegularization term implementing the proximal map for TV regularization. Calculated with the Condat algorithm if the TV is calculated only along one real-valued dimension and with the Fast Gradient Projection algorithm otherwise.\n\nReference for the Condat algorithm: https://lcondat.github.io/publis/Condat-fast_TV-SPL-2013.pdf\n\nReference for the FGP algorithm: A. Beck and T. Teboulle, \"Fast Gradient-Based Algorithms for Constrained Total Variation Image Denoising and Deblurring Problems\", IEEE Trans. Image Process. 18(11), 2009\n\nArguments\n\nλ::T                    - regularization parameter\n\nKeywords\n\nshape::NTuple           - size of the underlying image\ndims                    - Dimension to perform the TV along. If Integer, the Condat algorithm is called, and the FDG algorithm otherwise.\niterationsTV=20         - number of FGP iterations\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.PositiveRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.PositiveRegularization","text":"PositiveRegularization\n\nRegularization term implementing a projection onto positive and real numbers.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.RealRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.RealRegularization","text":"RealRegularization\n\nRegularization term implementing a projection onto real numbers.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.innerreg-Tuple{AbstractNestedRegularization}","page":"Regularization Terms","title":"RegularizedLeastSquares.innerreg","text":"innerreg(reg::AbstractNestedRegularization)\n\nreturn the inner regularization term of reg. Nested regularization terms also implement the iteration interface.\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#RegularizedLeastSquares.sink-Tuple{AbstractNestedRegularization}","page":"Regularization Terms","title":"RegularizedLeastSquares.sink","text":"sink(reg::AbstractNestedRegularization)\n\nreturn the innermost regularization term.\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#RegularizedLeastSquares.sinktype-Tuple{AbstractNestedRegularization}","page":"Regularization Terms","title":"RegularizedLeastSquares.sinktype","text":"sinktype(reg::AbstractNestedRegularization)\n\nreturn the type of the innermost regularization term.\n\nSee also sink.\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#RegularizedLeastSquares.AbstractScaledRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.AbstractScaledRegularization","text":"AbstractScaledRegularization\n\nNested regularization term that applies a scalefactor to the regularization parameter λ of its inner term.\n\nSee also scalefactor, λ, innerreg.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.scalefactor","page":"Regularization Terms","title":"RegularizedLeastSquares.scalefactor","text":"scalescalefactor(reg::AbstractScaledRegularization)\n\nreturn the scaling scalefactor for λ\n\n\n\n\n\n","category":"function"},{"location":"API/regularization/#RegularizedLeastSquares.NormalizedRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.NormalizedRegularization","text":"NormalizedRegularization\n\nNested regularization term that scales λ according to normalization scheme. This term is commonly applied by a solver based on a given normalization keyword\n\n#See also NoNormalization, MeasurementBasedNormalization, SystemMatrixBasedNormalization.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.NoNormalization","page":"Regularization Terms","title":"RegularizedLeastSquares.NoNormalization","text":"NoNormalization\n\nNo normalization to λ is applied.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.MeasurementBasedNormalization","page":"Regularization Terms","title":"RegularizedLeastSquares.MeasurementBasedNormalization","text":"MeasurementBasedNormalization\n\nλ is normalized by the 1-norm of b divided by its length.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.SystemMatrixBasedNormalization","page":"Regularization Terms","title":"RegularizedLeastSquares.SystemMatrixBasedNormalization","text":"SystemMatrixBasedNormalization\n\nλ is normalized by the energy of the system matrix rows.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.FixedParameterRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.FixedParameterRegularization","text":"FixedParameterRegularization\n\nNested regularization term that discards any λ passed to it and instead uses λ from its inner regularization term. This can be used to selectively disallow normalization.\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.MaskedRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.MaskedRegularization","text":"MaskedRegularization\n\nNested regularization term that only applies prox! and norm to elements of x for which the mask is true.\n\nExamples\n\njulia> positive = PositiveRegularization();\n\njulia> masked = MaskedRegularization(reg, [true, false, true, false]);\n\njulia> prox!(masked, fill(-1, 4))\n4-element Vector{Float64}:\n  0.0\n -1.0\n  0.0\n -1.0\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.TransformedRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.TransformedRegularization","text":"TransformedRegularization(reg, trafo)\n\nNested regularization term that applies prox! or norm on z = trafo * x and returns (inplace) x = adjoint(trafo) * z.\n\nExample\n\njulia> core = L1Regularization(0.8)\nL1Regularization{Float64}(0.8)\n\njulia> wop = WaveletOp(Float32, shape = (32,32));\n\njulia> reg = TransformedRegularization(core, wop);\n\njulia> prox!(reg, randn(32*32)); # Apply soft-thresholding in Wavelet domain\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.PlugAndPlayRegularization","page":"Regularization Terms","title":"RegularizedLeastSquares.PlugAndPlayRegularization","text":"    PlugAndPlayRegularization\n\nRegularization term implementing a given plug-and-play proximal mapping. The actual regularization term is indirectly defined by the learned proximal mapping and as such there is no norm implemented.\n\nArguments\n\nλ                  - regularization paramter\n\nKeywords\n\nmodel       - model applied to the image\nshape       - dimensions of the image\ninput_transform - transform of image before model\n\n\n\n\n\n","category":"type"},{"location":"API/regularization/#RegularizedLeastSquares.prox!-Tuple{AbstractParameterizedRegularization, AbstractArray}","page":"Regularization Terms","title":"RegularizedLeastSquares.prox!","text":"prox!(reg::AbstractParameterizedRegularization, x)\n\nperform the proximal mapping defined by reg on x. Uses the regularization parameter defined for reg.\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#RegularizedLeastSquares.prox!-Tuple{Type{<:AbstractParameterizedRegularization}, Any, Any}","page":"Regularization Terms","title":"RegularizedLeastSquares.prox!","text":"prox!(regType::Type{<:AbstractParameterizedRegularization}, x, λ; kwargs...)\n\nconstruct a regularization term of type regType with given λ and kwargs and apply its prox! on x\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#LinearAlgebra.norm-Tuple{AbstractParameterizedRegularization, AbstractArray}","page":"Regularization Terms","title":"LinearAlgebra.norm","text":"norm(reg::AbstractParameterizedRegularization, x)\n\nreturns the value of the reg regularization term on x. Uses the regularization parameter defined for reg.\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#RegularizedLeastSquares.λ-Tuple{AbstractParameterizedRegularization}","page":"Regularization Terms","title":"RegularizedLeastSquares.λ","text":"λ(reg::AbstractParameterizedRegularization)\n\nreturn the regularization parameter λ of reg\n\n\n\n\n\n","category":"method"},{"location":"API/regularization/#LinearAlgebra.norm-Tuple{Type{<:AbstractParameterizedRegularization}, Any, Any}","page":"Regularization Terms","title":"LinearAlgebra.norm","text":"norm(regType::Type{<:AbstractParameterizedRegularization}, x, λ; kwargs...)\n\nconstruct a regularization term of type regType with given λ and kwargs and apply its norm on x\n\n\n\n\n\n","category":"method"},{"location":"generated/howto/callbacks/#Callbacks","page":"Callbacks","title":"Callbacks","text":"For certain optimization it is important to monitor the internal state of the solver. RegularizedLeastSquares.jl provides a callback mechanism to allow developres to access this state after each iteration. The package provides a variety of default callbacks, which for example store the solution after each iteration. More information can be found in the API reference for the solve! function.\n\nIn this example we will revist the compressed sensing compressed sensing example and implement a custom callback using the do-syntax of the solve! function. This allows us to implement our callback inline and access the solver state after each iteration.\n\nWe first recreate the operator A and the measurement vector b:\n\nusing ImagePhantoms, ImageGeoms\nN = 256\nimage = shepp_logan(N, SheppLoganToft())\nsize(image)\nusing Random, LinearOperatorCollection\nrandomIndices = shuffle(eachindex(image))\nsampledIndices = sort(randomIndices[1:div(end, 3)])\nA = SamplingOp(eltype(image), pattern = sampledIndices , shape = size(image));\nb = A*vec(image);\nnothing #hide\n\nNext we prepare our visualization helper function:\n\nusing CairoMakie\nfunction plot_image(figPos, img; title = \"\", width = 150, height = 150, clim = extrema(img))\n  ax = CairoMakie.Axis(figPos; yreversed=true, title, width, height)\n  hidedecorations!(ax)\n  heatmap!(ax, img, colorrange = clim)\nend\n\nNow we construct the solver with the TV regularization term:\n\nusing RegularizedLeastSquares\nreg = TVRegularization(0.01; shape=size(image));\nsolver = createLinearSolver(FISTA, A; reg=reg, iterations=20);\nnothing #hide\n\nWe will now implement a callback that plots the solution every four iteration:\n\nfig = Figure()\nidx = 1\nsolve!(solver, b) do solver, iteration\n  if iteration % 4 == 0\n    img_approx = copy(solversolution(solver))\n    img_approx = reshape(img_approx, size(image))\n    plot_image(fig[div(idx -1, 3) + 1, mod1(idx, 3)], img_approx, clim = extrema(image), title = \"$iteration\")\n    global idx += 1\n  end\nend\n\nIn the callback we have to copy the solution, as the solver will update it in place. As is explained in the solver section, each features fields that are intended to be immutable during a solve! call and a state that is modified in each iteration. Depending on the solvers parameters and the measurement input, the state can differ in its fields and their type. Ideally, one tries to avoid accessing the state directly and uses the provided functions to access the state.\n\nThe resulting figure shows the reconstructed image after 0, 4, 8, 12, 16 and 20 iterations:\n\nresize_to_layout!(fig)\nfig\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/howto/normal_operator/#Normal-Operator","page":"Normal Operator","title":"Normal Operator","text":"Many solvers in RegularizedLeastSquares.jl are based on the normal operator mathbfA^*mathbfA of the linear operator mathbfA.\n\nSolvers such as ADMM, FISTA and POGM generally solve optimization problems of the form:\n\nbeginequation\n  undersetmathbfxargmin mathbff(x)+ mathbfR(x)\nendequation\n\nand require the gradient of the function mathbff(x). In this package we specialise the function mathbff(x) to the least squares norm:\n\nbeginequation\n  mathbff(x) = frac12vertvert mathbfAmathbfx-mathbfb vertvert^2_2\nendequation\n\nThe gradient of this function is:\n\nbeginequation\n  nabla mathbff(x) = mathbfA^*(mathbfAmathbfx-mathbfb) = mathbfA^*mathbfAx - mathbfA^*mathbfb\nendequation\n\nSimilarily, the conjugate gradient normal residual (CGNR) algorithm applies conjugate gradient algorithm to:\n\nbeginequation\n  mathbfA^*mathbfAmathbfx = mathbfA^*mathbfb\nendequation\n\nThe normal operator can be passed directly to these solvers, otherwise it is computed internally.\n\nusing RegularizedLeastSquares\nA = randn(32, 16)\nx = randn(16)\nb = A*x\n\nsolver = createLinearSolver(CGNR, A; AHA = adjoint(A) * A, reg = L2Regularization(0.0001), iterations=32);\nx_approx = solve!(solver, b)\n\nThe normal operator can also be computed using the normalOperator function from LinearOperatorCollection.jl. This is useful if the normal operator is not directly available or shouldn't be stored in memory. This function is opinionated and attempts to optimize the resulting operator for iterative applications. Specifying a custom method for a custom operator allows one to control this optimization.\n\nAn example of such an optimization is a matrix-free weighting of mathbfA as shown in the Weighting example:\n\nusing LinearOperatorCollection\nweights = rand(32)\nWA = ProdOp(WeightingOp(weights), A)\nAHA = LinearOperatorCollection.normalOperator(WA)\n\nWithout an optimization a matrix-free product would apply the following operator each iteration:\n\nbeginequation\n  (mathbfWA)^*mathbfWA = mathbfA^*mathbfW^*mathbfWmathbfA\nendequation\n\nThis is not efficient and instead the normal operator can be optimized by initially computing the weights:\n\nbeginequation\n  tildemathbfW = mathbfW^*mathbfW\nendequation\n\nand then applying the following each iteration:\n\nbeginequation\n  mathbfA^*tildemathbfWmathbfA\nendequation\n\nThe optimized normal operator can then be passed to the solver:\n\nsolver = createLinearSolver(CGNR, WA; AHA = AHA, reg = L2Regularization(0.0001), iterations=32);\nx_approx2 = solve!(solver, weights .* b)\n\nOf course it is also possible to optimize a normal operator with other means and pass it to the solver via the AHA keyword argument.\n\nIt is also possible to only supply the normal operator to these solvers, however on then needs to supply mathbfA^*b intead of mathbfb.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/howto/plug-and-play/#Plug-and-Play-Regularization","page":"Plug-and-Play Regularization","title":"Plug-and-Play Regularization","text":"A group of regularization terms that can not be directly written down as function are learned plug-and-play (PnP) priors. These are terms based on deep neural networks, which are trainted to implement the proximal map corresponding to the regularization term. Such a PnP prior can be used in the same way as any other regularization term.\n\nThe following example shows how to use a PnP prior in the context of the Kaczmarz solver.\n\nusing RegularizedLeastSquares\nA = randn(32, 16)\nx = randn(16)\nb = A*x;\nnothing #hide\n\nFor the documentation we will just use the identity function as a placeholder for the PnP prior.\n\nmodel = identity\n\nIn practice, you would replace this with a neural network:\n\nusing Flux\nmodel = Flux.loadmodel!(model, ...)\n\nThe model can then be used together with the PnPRegularization term:\n\nreg = PnPRegularization(1.0; model = model, shape = [16]);\nnothing #hide\n\nSince models often expect a specific input range, we can use the MinMaxTransform to normalize the input:\n\nreg = PnPRegularization(1.0; model = model, shape = [16], input_transform = RegularizedLeastSquares.MinMaxTransform);\nnothing #hide\n\nCustom input transforms can be implemented by passing something callable as the input_transform keyword argument. For more details see the PnPRegularization documentation.\n\nThe regularization term can then be used in the solver:\n\nsolver = createLinearSolver(Kaczmarz, A; reg = reg, iterations = 32)\nx_approx = solve!(solver, b)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"API/solvers/#API-for-Solvers","page":"Solvers","title":"API for Solvers","text":"This page contains documentation of the public API of the RegularizedLeastSquares. In the Julia REPL one can access this documentation by entering the help mode with ?","category":"section"},{"location":"API/solvers/#solve!","page":"Solvers","title":"solve!","text":"","category":"section"},{"location":"API/solvers/#ADMM","page":"Solvers","title":"ADMM","text":"","category":"section"},{"location":"API/solvers/#CGNR","page":"Solvers","title":"CGNR","text":"","category":"section"},{"location":"API/solvers/#Kaczmarz","page":"Solvers","title":"Kaczmarz","text":"","category":"section"},{"location":"API/solvers/#FISTA","page":"Solvers","title":"FISTA","text":"","category":"section"},{"location":"API/solvers/#OptISTA","page":"Solvers","title":"OptISTA","text":"","category":"section"},{"location":"API/solvers/#POGM","page":"Solvers","title":"POGM","text":"","category":"section"},{"location":"API/solvers/#SplitBregman","page":"Solvers","title":"SplitBregman","text":"","category":"section"},{"location":"API/solvers/#Miscellaneous","page":"Solvers","title":"Miscellaneous","text":"","category":"section"},{"location":"API/solvers/#RegularizedLeastSquares.solve!-Tuple{AbstractLinearSolver, Any}","page":"Solvers","title":"RegularizedLeastSquares.solve!","text":"solve!(solver::AbstractLinearSolver, b; x0 = 0, callbacks = (_, _) -> nothing)\n\nSolves an inverse problem for the data vector b using solver.\n\nRequired Arguments\n\nsolver::AbstractLinearSolver    - linear solver (e.g., ADMM or FISTA), containing forward/normal operator and regularizer\nb::AbstractVector               - data vector if A was supplied to the solver, back-projection of the data otherwise\n\nOptional Keyword Arguments\n\nx0::AbstractVector              - initial guess for the solution; default is zero\ncallbacks              - (optionally a vector of) function or callable struct that takes the two arguments callback(solver, iteration) and, e.g., stores, prints, or plots the intermediate solutions or convergence parameters. Be sure not to modify solver or iteration in the callback function as this would japaridze convergence. The default does nothing.\n\nExamples\n\nThe optimization problem\n\n\targmin_x Ax - b_2^2 + λ x_1\n\ncan be solved with the following lines of code:\n\njulia> using RegularizedLeastSquares\n\njulia> A = [0.831658  0.96717\n            0.383056  0.39043\n            0.820692  0.08118];\n\njulia> x = [0.5932234523399985; 0.2697534345340015];\n\njulia> b = A * x;\n\njulia> S = ADMM(A, reg = L1Regularization(0.0001));\n\njulia> x_approx = solve!(S, b)\n2-element Vector{Float64}:\n 0.5932171509222105\n 0.26971370566079866\n\nHere, we use L1Regularization. All regularization options can be found in API for Regularizers.\n\nThe following example solves the same problem, but stores the solution x of each interation in tr:\n\njulia> tr = Dict[]\nDict[]\n\njulia> store_trace!(tr, solver, iteration) = push!(tr, Dict(\"iteration\" => iteration, \"x\" => solver.x, \"beta\" => solver.β))\nstore_trace! (generic function with 1 method)\n\njulia> x_approx = solve!(S, b; callbacks=(solver, iteration) -> store_trace!(tr, solver, iteration))\n2-element Vector{Float64}:\n 0.5932234523399984\n 0.26975343453400163\n\njulia> tr[3]\nDict{String, Any} with 3 entries:\n  \"iteration\" => 2\n  \"x\"         => [0.593223, 0.269753]\n  \"beta\"      => [1.23152, 0.927611]\n\nThe last example show demonstrates how to plot the solution at every 10th iteration and store the solvers convergence metrics:\n\njulia> using Plots\n\njulia> conv = StoreConvergenceCallback()\n\njulia> function plot_trace(solver, iteration)\n         if iteration % 10 == 0\n           display(scatter(solver.x))\n         end\n       end\nplot_trace (generic function with 1 method)\n\njulia> x_approx = solve!(S, b; callbacks = [conv, plot_trace]);\n\nThe keyword callbacks allows you to pass a (vector of) callable objects that takes the arguments solver and iteration and prints, stores, or plots intermediate result.\n\nSee also StoreSolutionCallback, StoreConvergenceCallback, CompareSolutionCallback for a number of provided callback options.\n\n\n\n\n\n","category":"method"},{"location":"API/solvers/#RegularizedLeastSquares.init!-Tuple{AbstractLinearSolver, Any}","page":"Solvers","title":"RegularizedLeastSquares.init!","text":"init!(solver::AbstractLinearSolver, b; kwargs...)\n\nPrepare the solver for iteration based on the given data vector b and kwargs.\n\n\n\n\n\n","category":"method"},{"location":"API/solvers/#RegularizedLeastSquares.init!-Tuple{AbstractLinearSolver, AbstractSolverState, AbstractMatrix}","page":"Solvers","title":"RegularizedLeastSquares.init!","text":"init!(solver::AbstractLinearSolver, state::AbstractSolverState, b::AbstractMatrix; scheduler = SequentialState, kwargs...)\n\nInitialize the solver with each column of b and pass the corresponding states to the scheduler.\n\n\n\n\n\n","category":"method"},{"location":"API/solvers/#RegularizedLeastSquares.ADMM","page":"Solvers","title":"RegularizedLeastSquares.ADMM","text":"ADMM(A; AHA = A'*A, precon = Identity(), reg = L1Regularization(zero(real(eltype(AHA)))), regTrafo = opEye(eltype(AHA), size(AHA,1)), normalizeReg = NoNormalization(), rho = 1e-1, vary_rho = :none, iterations = 10, iterationsCG = 10, absTol = eps(real(eltype(AHA))), relTol = eps(real(eltype(AHA))), tolInner = 1e-5, verbose = false)\nADMM( ; AHA = ,     precon = Identity(), reg = L1Regularization(zero(real(eltype(AHA)))), regTrafo = opEye(eltype(AHA), size(AHA,1)), normalizeReg = NoNormalization(), rho = 1e-1, vary_rho = :none, iterations = 10, iterationsCG = 10, absTol = eps(real(eltype(AHA))), relTol = eps(real(eltype(AHA))), tolInner = 1e-5, verbose = false)\n\nCreates an ADMM object for the forward operator A or normal operator AHA.\n\nRequired Arguments\n\nA                                                 - forward operator\n\nOR\n\nAHA                                               - normal operator (as a keyword argument)\n\nOptional Keyword Arguments\n\nAHA                                               - normal operator is optional if A is supplied\nprecon                                            - preconditionner for the internal CG algorithm\nreg::AbstractParameterizedRegularization          - regularization term; can also be a vector of regularization terms\nregTrafo                                          - transformation to a space in which reg is applied; if reg is a vector, regTrafo has to be a vector of the same length. Use opEye(eltype(AHA), size(AHA,1)) if no transformation is desired.\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\nrho::Real                                         - penalty of the augmented Lagrangian\nvary_rho::Symbol                                  - vary rho to balance primal and dual feasibility; options :none, :balance, :PnP\niterations::Int                                   - maximum number of (outer) ADMM iterations\niterationsCG::Int                                 - maximum number of (inner) CG iterations\nabsTol::Real                                      - absolute tolerance for stopping criterion\nrelTol::Real                                      - relative tolerance for stopping criterion\ntolInner::Real                                    - relative tolerance for CG stopping criterion\nverbose::Bool                                     - print residual in each iteration\n\nADMM differs from ISTA-type algorithms in the sense that the proximal operation is applied separately from the transformation to the space in which the penalty is applied. This is reflected by the interface which has reg and regTrafo as separate arguments. E.g., for a TV penalty, you should NOT set reg=TVRegularization, but instead use reg=L1Regularization(λ), regTrafo=RegularizedLeastSquares.GradientOp(Float64; shape=(Nx,Ny,Nz)).\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.CGNR","page":"Solvers","title":"RegularizedLeastSquares.CGNR","text":"CGNR(A; AHA = A' * A, reg = L2Regularization(zero(real(eltype(AHA)))), normalizeReg = NoNormalization(), iterations = 10, relTol = eps(real(eltype(AHA))))\nCGNR( ; AHA = ,       reg = L2Regularization(zero(real(eltype(AHA)))), normalizeReg = NoNormalization(), iterations = 10, relTol = eps(real(eltype(AHA))))\n\ncreates an CGNR object for the forward operator A or normal operator AHA.\n\nRequired Arguments\n\nA                                                 - forward operator\n\nOR\n\nAHA                                               - normal operator (as a keyword argument)\n\nOptional Keyword Arguments\n\nAHA                                               - normal operator is optional if A is supplied\nreg::AbstractParameterizedRegularization          - regularization term; can also be a vector of regularization terms\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\niterations::Int                                   - maximum number of iterations\nrelTol::Real                                      - tolerance for stopping criterion\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.Kaczmarz","page":"Solvers","title":"RegularizedLeastSquares.Kaczmarz","text":"Kaczmarz(A; reg = L2Regularization(0), normalizeReg = NoNormalization(), randomized=false, subMatrixFraction=0.15, shuffleRows=false, seed=1234, iterations=10, greedy_randomized=false, theta=Nothing)\n\nCreates a Kaczmarz object for the forward operator A.\n\nRequired Arguments\n\nA                                                 - forward operator\n\nOptional Keyword Arguments\n\nreg::AbstractParameterizedRegularization          - regularization term\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\nrandomized::Bool                                    - randomize Kacmarz algorithm\nsubMatrixFraction::Real                             - fraction of rows used in randomized Kaczmarz algorithm\nshuffleRows::Bool                                   - randomize Kacmarz algorithm\nseed::Int                                           - seed for randomized algorithm\niterations::Int                                     - number of iterations\ngreedy_randomized::Bool                             - use greedy randomized kaczmarz\ntheta::Float64                                      - set relaxation parameter theta\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.FISTA","page":"Solvers","title":"RegularizedLeastSquares.FISTA","text":"FISTA(A; AHA=A'*A, reg=L1Regularization(zero(real(eltype(AHA)))), normalizeReg=NoNormalization(), iterations=50, verbose = false, rho = 0.95 / power_iterations(AHA), theta=1, relTol=eps(real(eltype(AHA))), restart = :none)\nFISTA( ; AHA=,     reg=L1Regularization(zero(real(eltype(AHA)))), normalizeReg=NoNormalization(), iterations=50, verbose = false, rho = 0.95 / power_iterations(AHA), theta=1, relTol=eps(real(eltype(AHA))), restart = :none)\n\ncreates a FISTA object for the forward operator A or normal operator AHA.\n\nRequired Arguments\n\nA                                                 - forward operator\n\nOR\n\nAHA                                               - normal operator (as a keyword argument)\n\nOptional Keyword Arguments\n\nAHA                                               - normal operator is optional if A is supplied\nprecon                                            - preconditionner for the internal CG algorithm\nreg::AbstractParameterizedRegularization          - regularization term; can also be a vector of regularization terms\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\nrho::Real                                         - step size for gradient step; the default is 0.95 / max_eigenvalue as determined with power iterations.\ntheta::Real                                       - parameter for predictor-corrector step\nrelTol::Real                                      - tolerance for stopping criterion\niterations::Int                                   - maximum number of iterations\nrestart::Symbol                                   - :none, :gradient options for restarting\nverbose::Bool                                     - print residual in each iteration\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.OptISTA","page":"Solvers","title":"RegularizedLeastSquares.OptISTA","text":"OptISTA(A; AHA=A'*A, reg=L1Regularization(zero(real(eltype(AHA)))), normalizeReg=NoNormalization(), iterations=50, verbose = false, rho=0.95 / power_iterations(AHA), theta=1, relTol=eps(real(eltype(AHA))))\nOptISTA( ; AHA=,     reg=L1Regularization(zero(real(eltype(AHA)))), normalizeReg=NoNormalization(), iterations=50, verbose = false, rho=0.95 / power_iterations(AHA), theta=1, relTol=eps(real(eltype(AHA))))\n\ncreates a OptISTA object for the forward operator A or normal operator AHA. OptISTA has a 2x better worst-case bound than FISTA, but actual performance varies by application. It stores 2 extra intermediate variables the size of the image compared to FISTA.\n\nReference:\n\nUijeong Jang, Shuvomoy Das Gupta, Ernest K. Ryu, \"Computer-Assisted Design of Accelerated Composite Optimization Methods: OptISTA,\" arXiv:2305.15704, 2023, [https://arxiv.org/abs/2305.15704]\n\nRequired Arguments\n\nA                                                 - forward operator\n\nOR\n\nAHA                                               - normal operator (as a keyword argument)\n\nOptional Keyword Arguments\n\nAHA                                               - normal operator is optional if A is supplied\nreg::AbstractParameterizedRegularization          - regularization term\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\nrho::Real                                         - step size for gradient step; the default is 0.95 / max_eigenvalue as determined with power iterations.\ntheta::Real                                       - parameter for predictor-corrector step\nrelTol::Real                                      - tolerance for stopping criterion\niterations::Int                                   - maximum number of iterations\nverbose::Bool                                     - print residual in each iteration\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.POGM","page":"Solvers","title":"RegularizedLeastSquares.POGM","text":"POGM(A; AHA = A'*A, reg = L1Regularization(zero(real(eltype(AHA)))), normalizeReg = NoNormalization(), iterations = 50, verbose = false, rho = 0.95 / power_iterations(AHA), theta = 1, sigma_fac = 1, relTol = eps(real(eltype(AHA))), restart = :none)\nPOGM( ; AHA = ,     reg = L1Regularization(zero(real(eltype(AHA)))), normalizeReg = NoNormalization(), iterations = 50, verbose = false, rho = 0.95 / power_iterations(AHA), theta = 1, sigma_fac = 1, relTol = eps(real(eltype(AHA))), restart = :none)\n\nCreates a POGM object for the forward operator A or normal operator AHA. POGM has a 2x better worst-case bound than FISTA, but actual performance varies by application. It stores 3 extra intermediate variables the size of the image compared to FISTA. Only gradient restart scheme is implemented for now.\n\nReferences:\n\nA.B. Taylor, J.M. Hendrickx, F. Glineur,   \"Exact worst-case performance of first-order algorithms   for composite convex optimization,\" Arxiv:1512.07516, 2015,   SIAM J. Opt. 2017   [http://doi.org/10.1137/16m108104x]\nKim, D., & Fessler, J. A. (2018).   Adaptive Restart of the Optimized Gradient Method for Convex Optimization.   Journal of Optimization Theory and Applications, 178(1), 240–263.   [https://doi.org/10.1007/s10957-018-1287-4]\nRequired Arguments\nA                                                 - forward operator\nOR\nAHA                                               - normal operator (as a keyword argument)\nOptional Keyword Arguments\nAHA                                               - normal operator is optional if A is supplied\nreg::AbstractParameterizedRegularization          - regularization term\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\nrho::Real                                         - step size for gradient step; the default is 0.95 / max_eigenvalue as determined with power iterations.\ntheta::Real                                       - parameter for predictor-corrector step\nsigma_fac::Real                                   - parameter for decreasing γ-momentum ∈ [0,1]\nrelTol::Real                                      - tolerance for stopping criterion\niterations::Int                                   - maximum number of iterations\nrestart::Symbol                                   - :none, :gradient options for restarting\nverbose::Bool                                     - print residual in each iteration\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.SplitBregman","page":"Solvers","title":"RegularizedLeastSquares.SplitBregman","text":"SplitBregman(A; AHA = A'*A, precon = Identity(), reg = L1Regularization(zero(real(eltype(AHA)))), regTrafo = opEye(eltype(AHA), size(AHA,1)), normalizeReg = NoNormalization(), rho = 1e-1, iterations = 10, iterationsInner = 10, iterationsCG = 10, absTol = eps(real(eltype(AHA))), relTol = eps(real(eltype(AHA))), tolInner = 1e-5, verbose = false)\nSplitBregman( ; AHA = ,     precon = Identity(), reg = L1Regularization(zero(real(eltype(AHA)))), regTrafo = opEye(eltype(AHA), size(AHA,1)), normalizeReg = NoNormalization(), rho = 1e-1, iterations = 10, iterationsInner = 10, iterationsCG = 10, absTol = eps(real(eltype(AHA))), relTol = eps(real(eltype(AHA))), tolInner = 1e-5, verbose = false)\n\nCreates a SplitBregman object for the forward operator A or normal operator AHA.\n\nRequired Arguments\n\nA                                                 - forward operator\n\nOR\n\nAHA                                               - normal operator (as a keyword argument)\n\nOptional Keyword Arguments\n\nAHA                                               - normal operator is optional if A is supplied\nprecon                                            - preconditionner for the internal CG algorithm\nreg::AbstractParameterizedRegularization          - regularization term; can also be a vector of regularization terms\nregTrafo                                          - transformation to a space in which reg is applied; if reg is a vector, regTrafo has to be a vector of the same length. Use opEye(eltype(AHA), size(AHA,1)) if no transformation is desired.\nnormalizeReg::AbstractRegularizationNormalization - regularization normalization scheme; options are NoNormalization(), MeasurementBasedNormalization(), SystemMatrixBasedNormalization()\nrho::Real                                         - weights for condition on regularized variables; can also be a vector for multiple regularization terms\niterations::Int                              - maximum number of outer iterations. Set to 1 for unconstraint split Bregman (equivalent to ADMM)\niterationsInner::Int                              - maximum number of inner iterations\niterationsCG::Int                                 - maximum number of (inner) CG iterations\nabsTol::Real                                      - absolute tolerance for stopping criterion\nrelTol::Real                                      - relative tolerance for stopping criterion\ntolInner::Real                                    - relative tolerance for CG stopping criterion\nverbose::Bool                                     - print residual in each iteration\n\nThis algorithm solves the constraint problem (Eq. (4.7) in Tom Goldstein and Stanley Osher), i.e. ||R(x)||₁ such that ||Ax -b||₂² < σ². In order to solve the unconstraint problem (Eq. (4.8) in Tom Goldstein and Stanley Osher), i.e. ||Ax -b||₂² + λ ||R(x)||₁, you can either set iterations=1 or use ADMM instead, which is equivalent (iterations=1 in SplitBregman in implied in ADMM and the SplitBregman variable iterationsInner is simply called iterations in ADMM)\n\nLike ADMM, SplitBregman differs from ISTA-type algorithms in the sense that the proximal operation is applied separately from the transformation to the space in which the penalty is applied. This is reflected by the interface which has reg and regTrafo as separate arguments. E.g., for a TV penalty, you should NOT set reg=TVRegularization, but instead use reg=L1Regularization(λ), regTrafo=RegularizedLeastSquares.GradientOp(Float64; shape=(Nx,Ny,Nz)).\n\nSee also createLinearSolver, solve!.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.solverstate","page":"Solvers","title":"RegularizedLeastSquares.solverstate","text":"solverstate(solver::AbstractLinearSolver)\n\nReturn the current state of the solver\n\n\n\n\n\n","category":"function"},{"location":"API/solvers/#RegularizedLeastSquares.solversolution","page":"Solvers","title":"RegularizedLeastSquares.solversolution","text":"solversolution(solver::AbstractLinearSolver)\n\nReturn the current solution of the solver\n\n\n\n\n\nsolversolution(state::AbstractSolverState)\n\nReturn the current solution of the solver's state\n\n\n\n\n\n","category":"function"},{"location":"API/solvers/#RegularizedLeastSquares.solverconvergence","page":"Solvers","title":"RegularizedLeastSquares.solverconvergence","text":"solverconvergence(solver::AbstractLinearSolver)\n\nReturn a named tuple of the solvers current convergence metrics\n\n\n\n\n\n","category":"function"},{"location":"API/solvers/#RegularizedLeastSquares.StoreSolutionCallback","page":"Solvers","title":"RegularizedLeastSquares.StoreSolutionCallback","text":"StoreSolutionCallback(T)\n\nCallback that accumlates the solvers solution per iteration. Results are stored in the solutions field.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.StoreConvergenceCallback","page":"Solvers","title":"RegularizedLeastSquares.StoreConvergenceCallback","text":"StoreConvergenceCallback()\n\nCallback that accumlates the solvers convergence metrics per iteration. Results are stored in the convMeas field.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.CompareSolutionCallback","page":"Solvers","title":"RegularizedLeastSquares.CompareSolutionCallback","text":"CompareSolutionCallback(ref, cmp)\n\nCallback that compares the solvers current solution with the given reference via cmp(ref, solution) per iteration. Results are stored in the results field.\n\n\n\n\n\n","category":"type"},{"location":"API/solvers/#RegularizedLeastSquares.linearSolverList","page":"Solvers","title":"RegularizedLeastSquares.linearSolverList","text":"Return a list of all available linear solvers\n\n\n\n\n\n","category":"function"},{"location":"API/solvers/#RegularizedLeastSquares.createLinearSolver","page":"Solvers","title":"RegularizedLeastSquares.createLinearSolver","text":"createLinearSolver(solver::AbstractLinearSolver, A; kargs...)\n\nThis method creates a solver. The supported solvers are methods typically used for solving regularized linear systems. All solvers return an approximate solution to Ax = b.\n\nTODO: give a hint what solvers are available\n\n\n\n\n\n","category":"function"},{"location":"API/solvers/#RegularizedLeastSquares.applicableSolverList","page":"Solvers","title":"RegularizedLeastSquares.applicableSolverList","text":"applicable(args...)\n\nlist all solvers that are applicable to the given arguments. Arguments are the same as for isapplicable without the solver type.\n\nSee also isapplicable, linearSolverList.\n\n\n\n\n\n","category":"function"},{"location":"API/solvers/#RegularizedLeastSquares.isapplicable","page":"Solvers","title":"RegularizedLeastSquares.isapplicable","text":"isapplicable(solverType::Type{<:AbstractLinearSolver}, A, x, reg)\n\nreturn true if a solver of type solverType is applicable to system matrix A, data x and regularization terms reg.\n\n\n\n\n\n","category":"function"},{"location":"generated/howto/gpu_acceleration/#GPU-Acceleration","page":"GPU Acceleration","title":"GPU Acceleration","text":"gpu = Array; #hide\nnothing #hide\n\nRegularizedLeastSquares.jl supports generic GPU acceleration. This means that the user can use any GPU array type that supports the GPUArrays interface. This includes CUDA.jl, AMDGPU.jl, and Metal.jl. To perform a reconstruction on the GPU, one has to load a GPU backend package such as CUDA and specify the GPU array type:\n\nusing CUDA\ngpu = CuArray\n\nAt first we will look at an example of dense GPU arrays.\n\nusing RegularizedLeastSquares\nA = gpu(rand(Float32, 32, 16))\nx = gpu(rand(Float32, 16))\nb = A*x;\nnothing #hide\n\nSolvers adapt their states based on the type of the given measurement vector. This means that the solver will automatically switch to GPU acceleration if a GPU array is passed as the measurement vector.\n\nsolver = createLinearSolver(CGNR, A; reg = L2Regularization(0.0001), iterations=32);\nx_approx = solve!(solver, b)\n\nThis adaption does not include the operator. So if we want to compare with CPU result, we need to construct a new solver with a CPU operator.\n\nsolver = createLinearSolver(CGNR, Array(A); reg = L2Regularization(0.0001), iterations=32);\nx_cpu = solve!(solver, Array(b))\nisapprox(Array(x_approx), x_cpu)","category":"section"},{"location":"generated/howto/gpu_acceleration/#Matrix-Free-Operators","page":"GPU Acceleration","title":"Matrix-Free Operators","text":"A special case is the usage of matrix-free operators. Since these operators do not have a concrete matrix representation, their GPU support depends on their implementation. Since not all multiplications within a solver approximation are in-place, the operator also needs to support the * operation and construct an appropriate result vector. For matrix-free operators based on LinearOperators.jl, this can be achieved by implementing the LinearOperators.storage_type method.\n\nIn the following, we will take another look at the CS example and execute it on the GPU. Note that for the JLArray example we chose a small phantom, since the JLArray implementation is not optimized for performance:\n\nusing ImagePhantoms, ImageGeoms\nN = 32\nimage = shepp_logan(N, SheppLoganToft())\n\nusing Random, LinearOperatorCollection\nrandomIndices = shuffle(eachindex(image))\nsampledIndices = sort(randomIndices[1:div(end, 3)]);\nnothing #hide\n\nTo construct the operator, we need to convert the indices to a GPU array. We also need to specify the correct storage type. In both LinearOperators.jl and LinearOperatorCollection.jl this is done with the S keyword argument.\n\ngpu_indices = gpu(sampledIndices)\nA = SamplingOp(eltype(image), pattern = gpu_indices, shape = size(image), S = typeof(b));\nnothing #hide\n\nLet's inspect the storage type of the operator:\n\nusing LinearOperatorCollection.LinearOperators\nLinearOperators.storage_type(A)\n\nAfterwards we can use the operator as usual:\n\nb = A*vec(gpu(image));\nnothing #hide\n\nAnd use it in the solver:\n\nusing RegularizedLeastSquares\nreg = TVRegularization(0.01; shape=size(image))\nsolver = createLinearSolver(FISTA, A; reg=reg, iterations=20)\nimg_approx = solve!(solver,b);\nnothing #hide\n\nTo visualize the reconstructed image, we need to reshape the result vector to the correct shape and convert it to a CPU array:\n\nimg_approx = reshape(Array(img_approx),size(image))\n\nWe will again use CairoMakie for visualization:\n\nusing CairoMakie\nfunction plot_image(figPos, img; title = \"\", width = 150, height = 150)\n  ax = CairoMakie.Axis(figPos; yreversed=true, title, width, height)\n  hidedecorations!(ax)\n  heatmap!(ax, img)\nend\nfig = Figure()\nplot_image(fig[1,1], image, title = \"Image\")\nsamplingMask = fill(false, size(image))\nsamplingMask[sampledIndices] .= true\nplot_image(fig[1,2], image .* samplingMask, title = \"Sampled Image\")\nplot_image(fig[1,3], img_approx, title = \"Reconstructed Image\")\nresize_to_layout!(fig)\nfig\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/examples/compressed_sensing/#Compressed-Sensing-Example","page":"Compressed Sensing","title":"Compressed Sensing Example","text":"In this example we will go through a simple example from the field of Compressed Sensing. In addtion to RegularizedLeastSquares.jl, we will need the packages LinearOperatorCollection.jl, ImagePhantoms.jl, ImageGeoms.jl and Random.jl, as well as CairoMakie.jl for visualization. We can install them the same way we did RegularizedLeastSquares.jl.","category":"section"},{"location":"generated/examples/compressed_sensing/#Preparing-the-Inverse-Problem","page":"Compressed Sensing","title":"Preparing the Inverse Problem","text":"To get started, let us generate a simple phantom using the ImagePhantoms and ImageGeom packages:\n\nusing ImagePhantoms, ImageGeoms\nN = 256\nimage = shepp_logan(N, SheppLoganToft())\nsize(image)\n\nThis produces a 256x256 image of a Shepp-Logan phantom.\n\nIn this example, we consider a problem in which we randomly sample a third of the pixels in the image. Such a problem and the corresponding measurement can be constructed with the packages LinearOperatorCollection and Random:\n\nWe first randomly shuffle the indices of the image and then select the first third of the indices to sample.\n\nusing Random, LinearOperatorCollection\nrandomIndices = shuffle(eachindex(image))\nsampledIndices = sort(randomIndices[1:div(end, 3)])\n\nAfterwards we build a sampling operator which samples the image at the selected indices.\n\nA = SamplingOp(eltype(image), pattern = sampledIndices , shape = size(image));\nnothing #hide\n\nThen we apply the sampling operator to the vectorized image to obtain the sampled measurement vector\n\nb = A*vec(image);\nnothing #hide\n\nTo visualize our image we can use CairoMakie:\n\nusing CairoMakie\nfunction plot_image(figPos, img; title = \"\", width = 150, height = 150)\n  ax = CairoMakie.Axis(figPos; yreversed=true, title, width, height)\n  hidedecorations!(ax)\n  heatmap!(ax, img)\nend\nfig = Figure()\nplot_image(fig[1,1], image, title = \"Image\")\nsamplingMask = fill(false, size(image))\nsamplingMask[sampledIndices] .= true\nplot_image(fig[1,2], image .* samplingMask, title = \"Sampled Image\")\nresize_to_layout!(fig)\nfig\n\nAs we can see in the right image, only a third of the pixels are sampled. The goal of the inverse problem is to recover the original image from this measurement vector.","category":"section"},{"location":"generated/examples/compressed_sensing/#Solving-the-Inverse-Problem","page":"Compressed Sensing","title":"Solving the Inverse Problem","text":"To recover the image from the measurement vector, we solve the TV-regularized least squares problem:\n\nbeginequation\n  undersetmathbfxargmin frac12vertvert mathbfAmathbfx-mathbfb vertvert_2^2 + lambdavertvertmathbfxvertvert_textTV \nendequation\n\nFor this purpose we build a TV regularizer with regularization parameter λ=001\n\nusing RegularizedLeastSquares\nreg = TVRegularization(0.01; shape=size(image));\nnothing #hide\n\nWe will use the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) to solve our inverse problem. Thus, we build the corresponding solver\n\nsolver = createLinearSolver(FISTA, A; reg=reg, iterations=20);\nnothing #hide\n\nand apply it to our measurement vector\n\nimg_approx = solve!(solver,b)\n\nTo visualize the reconstructed image, we need to reshape the result vector to the correct shape. Afterwards we can use CairoMakie again:\n\nimg_approx = reshape(img_approx,size(image));\nplot_image(fig[1,3], img_approx, title = \"Reconstructed Image\")\nresize_to_layout!(fig)\nfig\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/examples/getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"In this example we will go through a simple random inverse problem to get familiar with RegularizedLeastSquares.jl.","category":"section"},{"location":"generated/examples/getting_started/#Installation","page":"Getting Started","title":"Installation","text":"To install RegularizedLeastSquares.jl, you can use the Julia package manager. Open a Julia REPL and run the following command:\n\nusing Pkg\nPkg.add(\"RegularizedLeastSquares\")\n\nThis will download and install the RegularizedLeastSquares.jl package and its dependencies. To install a different version, please consult the Pkg documentation.\n\nOnce the installation is complete, you can import the package with the using keyword:\n\nusing RegularizedLeastSquares\n\nRegularizedLeastSquares aims to solve inverse problems of the form:\n\nbeginequation\n  undersetmathbfxargmin frac12vertvert mathbfAmathbfx-mathbfb vertvert_2^2 + mathbfR(x) \nendequation\n\nwhere mathbfA is a linear operator, mathbfb is the measurement vector, and mathbfR(x) is an (optional) regularization term. The goal is to retrieve an approximation of the unknown vector mathbfx. In this first exampel we will just work with simple random arrays. For more advanced examples, please refer to the examples.\n\nA = rand(32, 16)\nx = rand(16)\nb = A*x;\nnothing #hide\n\nTo approximate x from b, we can use the Conjugate Gradient Normal Residual (CGNR) algorithm. We first build the corresponding solver:\n\nsolver = createLinearSolver(CGNR, A; iterations=32);\nnothing #hide\n\nand apply it to our measurement vector\n\nx_approx = solve!(solver, b)\nisapprox(x, x_approx, rtol = 0.001)\n\nUsually the inverse problems are ill-posed and require regularization. RegularizedLeastSquares.jl provides a variety of regularization terms. The CGNR algorithm can solve optimzation problems of the form:\n\nbeginequation\n  undersetmathbfxargmin frac12vertvert mathbfAmathbfx-mathbfb vertvert_2^2 + lambdavertvertmathbfxvertvert^2_2 \nendequation\n\nThe corresponding solver can be built with the l^2_2-regularization term:\n\nsolver = createLinearSolver(CGNR, A; reg = L2Regularization(0.0001), iterations=32);\nx_approx = solve!(solver, b)\nisapprox(x, x_approx, rtol = 0.001)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"generated/explanations/regularization/#Regularization","page":"Regularization","title":"Regularization","text":"When formulating inverse problems, a regulariser is formulated as an additional cost function which has to be minimised. Many algorithms deal with a regularizers g, by computing the proximal map:\n\nbeginequation\n  prox_g (mathbfx) = undersetmathbfuargmin frac12vertvert mathbfu-mathbfx vert vert^2 + g(mathbfx)\nendequation\n\nFor many regularizers, the proximal map can be computed efficiently in a closed form.\n\nIn order to implement these proximal mappings, RegularizedLeastSquares.jl defines the following type hierarchy:\n\nabstract type AbstractRegularization\nprox!(reg::AbstractRegularization, x)\nnorm(reg::AbstractRegularization, x)\n\nHere prox!(reg, x) is an in-place function which computes the proximal map on the input-vector x. The function norm computes the value of the corresponding term in the inverse problem. RegularizedLeastSquares.jl provides AbstractParameterizedRegularization and AbstractProjectionRegularization as core regularization types.","category":"section"},{"location":"generated/explanations/regularization/#Parameterized-Regularization-Terms","page":"Regularization","title":"Parameterized Regularization Terms","text":"This group of regularization terms features a regularization parameter λ that is used during the prox! and normcomputations. Examples of this regulariztion group are L1, L2 or LLR (locally low rank) regularization terms.\n\nThese terms are constructed by supplying a λ and optionally term specific keyword arguments:\n\nusing RegularizedLeastSquares\nl2 = L2Regularization(0.3)\n\nParameterized regularization terms implement:\n\nprox!(reg::AbstractParameterizedRegularization, x, λ)\nnorm(reg::AbstractParameterizedRegularization, x, λ)\n\nwhere λ by default is filled with the value used during construction.\n\nInvoking λ on a parameterized term retrieves its regularization parameter. This can be used in a solver to scale and overwrite the parameter as follows:\n\nprox!(l2, [1.0])\n\nparam = λ(l2)\nprox!(l2, [1.0], param*0.2)","category":"section"},{"location":"generated/explanations/regularization/#Projection-Regularization-Terms","page":"Regularization","title":"Projection Regularization Terms","text":"This group of regularization terms implement projections, such as a positivity constraint or a projection with a given convex projection function. These are essentially proximal maps where g(mathbfx) is the indicator function of a convex set.\n\npositive = PositiveRegularization()\nprox!(positive, [2.0, -0.2])","category":"section"},{"location":"generated/explanations/regularization/#Nested-Regularization-Terms","page":"Regularization","title":"Nested Regularization Terms","text":"Nested regularization terms are terms that act as decorators to the core regularization terms. These terms can be nested around other terms and add functionality to a regularization term, such as scaling λ based on the provided operator or applying a transform, such as the Wavelet, to x. As an example, we can nest a L1 regularization term around a Wavelet operator.\n\nFirst we generate an image and apply a wavelet operator to it:\n\nusing Wavelets, LinearOperatorCollection, ImagePhantoms, ImageGeoms\nN = 256\nimage = shepp_logan(N, SheppLoganToft())\nwop = WaveletOp(Float32, shape = size(image))\n\nwavelet_image = reshape(wop*vec(image), size(image))\nwavelet_image = log.(abs.(wavelet_image) .+ 0.01)\n\nWe will use CairoMakie for visualization:\n\nusing CairoMakie\nfunction plot_image(figPos, img; title = \"\", width = 150, height = 150)\n  ax = CairoMakie.Axis(figPos; yreversed=true, title, width, height)\n  hidedecorations!(ax)\n  heatmap!(ax, img)\nend\nfig = Figure()\nplot_image(fig[1,1], image, title = \"Image\")\nplot_image(fig[1,2], wavelet_image, title = \"Wavelet Image\")\nresize_to_layout!(fig)\nfig\n\nTo apply soft-thresholding in the wavelet domain, we can nest a L1 regularization term around the Wavelet operator:\n\ncore = L1Regularization(0.1)\nreg = TransformedRegularization(core, wop);\nnothing #hide\n\nWe can then apply the proximal map to the image or the image in the wavelet domain:\n\nimg_prox_image = prox!(core, copy(vec(image)));\nimg_prox_wavelet = prox!(reg, copy(vec(image)));\nnothing #hide\n\nWe can visualize the result:\n\nimg_prox_image = reshape(img_prox_image, size(image))\nimg_prox_wavelet = reshape(img_prox_wavelet, size(image))\nplot_image(fig[1,3], img_prox_image, title = \"Reg. Image Domain\")\nplot_image(fig[1,4], img_prox_wavelet, title = \"Reg. Wavelet Domain\")\nresize_to_layout!(fig)\nfig\n\nGenerally, regularization terms can be nested arbitrarly deep, adding new functionality with each layer. Each nested regularization term can return its inner regularization term. Furthermore, all regularization terms implement the iteration interface to iterate over the nesting. The innermost regularization term of a nested term must be a core regularization term and it can be returned by the sink function:\n\nRegularizedLeastSquares.innerreg(reg) == core\n\nsink(reg) == core\n\nforeach(r -> println(nameof(typeof(r))), reg)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"#RegularizedLeastSquares.jl","page":"Home","title":"RegularizedLeastSquares.jl","text":"Solvers for Linear Inverse Problems using Regularization Techniques","category":"section"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"RegularizedLeastSquares.jl is a Julia package for solving large linear systems using various types of algorithms. Ill-conditioned problems arise in many areas of practical interest. Regularisation techniques and nonlinear problem formulations are often used to solve these problems. This package provides implementations for a variety of solvers used in areas such as MPI and MRI. In particular, this package serves as the optimization backend of the Julia packages MPIReco.jl and MRIReco.jl.\n\nThe implemented methods range from the l^2_2-regularized CGNR method to more general optimizers such as the Alternating Direction of Multipliers Method (ADMM) or the Split-Bregman method.\n\nFor convenience, implementations of popular regularizers, such as l_1-regularization and TV regularization, are provided. On the other hand, hand-crafted regularizers can be used quite easily.\n\nDepending on the problem, it becomes unfeasible to store the full system matrix at hand. For this purpose, RegularizedLeastSquares.jl allows for the use of matrix-free operators. Such operators can be realized using the interface provided by the package LinearOperators.jl. Other interfaces can be used as well, as long as the product *(A,x) and the adjoint adjoint(A) are provided. A number of common matrix-free operators are provided by the package LinearOperatorColection.jl.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Variety of optimization algorithms optimized for least squares problems\nSupport for matrix-free operators\nGPU support","category":"section"},{"location":"#Usage","page":"Home","title":"Usage","text":"See Getting Started for an introduction to using the package","category":"section"},{"location":"#See-also","page":"Home","title":"See also","text":"Packages:\n\nProximalAlgorithms.jl\nProximalOperators.jl\nKrylov.jl\nRegularizedOptimization.jl\n\nOrganizations:\n\nJuliaNLSolvers\nJuliaSmoothOptimizers\nJuliaFirstOrder","category":"section"}]
}
